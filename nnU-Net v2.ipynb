{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0162db-9f18-4398-b6a9-c30dd7f1d5fa",
   "metadata": {},
   "source": [
    "A script to train a nnU-Net V2 model, for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6155ce54-6421-4400-b429-221f02955746",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb98b9fa-bcdc-4f42-a818-37d5798df429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'available': True,\n",
       " 'count': 1,\n",
       " 'names': ['NVIDIA GeForce RTX 3060 Laptop GPU'],\n",
       " 'memory_gb': [5.99951171875],\n",
       " 'cuda_version': '12.6'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'platform': 'Linux',\n",
       " 'python_version': '3.10.12',\n",
       " 'torch_version': '2.7.0+cu126',\n",
       " 'cpu_count': 8,\n",
       " 'cpu_count_logical': 16,\n",
       " 'memory_gb': 7.6018829345703125}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.common import get_system_info, get_gpu_info\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "display(get_gpu_info())\n",
    "display(get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b99684-424b-4b4f-8ebd-63ca5986bf40",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62558c8-02d4-43e9-9f0b-e9cc67177a8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nnUNet'...\n",
      "remote: Enumerating objects: 14008, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 14008 (delta 0), reused 3 (delta 0), pack-reused 14005 (from 1)\u001b[K\n",
      "Receiving objects: 100% (14008/14008), 8.61 MiB | 6.60 MiB/s, done.\n",
      "Resolving deltas: 100% (10702/10702), done.\n",
      "Updating files: 100% (246/246), done.\n",
      "Obtaining file:///mnt/c/Users/abdal/Documents/Projects/ai-notebooks/nnUNet\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dynamic-network-architectures<0.5,>=0.4.1\n",
      "  Downloading dynamic_network_architectures-0.4.2.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting graphviz\n",
      "  Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: scipy in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (1.15.3)\n",
      "Collecting tifffile\n",
      "  Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.5/226.5 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting batchgenerators>=0.25.1\n",
      "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting acvl-utils<0.3,>=0.2.3\n",
      "  Downloading acvl_utils-0.2.5.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (4.67.1)\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (3.10.3)\n",
      "Collecting blosc2>=3.0.0b1\n",
      "  Downloading blosc2-3.10.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: requests in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (2.2.6)\n",
      "Collecting scikit-image>=0.19.3\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.1.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (2.7.0)\n",
      "Collecting imagecodecs\n",
      "  Downloading imagecodecs-2025.3.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (2.3.0)\n",
      "Requirement already satisfied: einops in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (0.8.1)\n",
      "Collecting yacs\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting batchgeneratorsv2>=0.3.0\n",
      "  Downloading batchgeneratorsv2-0.3.0.tar.gz (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from nnunetv2==2.6.2) (0.13.2)\n",
      "Collecting SimpleITK>=2.2.1\n",
      "  Downloading simpleitk-2.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (52.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting connected-components-3d\n",
      "  Downloading connected_components_3d-3.26.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=7.1.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from batchgenerators>=0.25.1->nnunetv2==2.6.2) (11.2.1)\n",
      "Requirement already satisfied: threadpoolctl in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from batchgenerators>=0.25.1->nnunetv2==2.6.2) (3.6.0)\n",
      "Collecting unittest2\n",
      "  Using cached unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "Collecting fft-conv-pytorch\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.1/406.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from blosc2>=3.0.0b1->nnunetv2==2.6.2) (4.3.8)\n",
      "Collecting ndindex\n",
      "  Downloading ndindex-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (501 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m501.6/501.6 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting numexpr>=2.14.1\n",
      "  Downloading numexpr-2.14.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.2/440.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting timm\n",
      "  Downloading timm-1.0.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=3.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (3.4.2)\n",
      "Requirement already satisfied: packaging>=21 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (25.0)\n",
      "Collecting lazy-loader>=0.4\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2==2.6.2) (2.37.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.80)\n",
      "Requirement already satisfied: fsspec in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (0.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.4.1)\n",
      "Requirement already satisfied: filelock in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (12.6.77)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.6.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.1.2->nnunetv2==2.6.2) (59.6.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (1.4.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (3.2.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (4.58.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.6.2) (2.9.0.post0)\n",
      "Collecting importlib-resources>=5.12\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from pandas->nnunetv2==2.6.2) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from pandas->nnunetv2==2.6.2) (2025.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from requests->nnunetv2==2.6.2) (2025.4.26)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from requests->nnunetv2==2.6.2) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from requests->nnunetv2==2.6.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from requests->nnunetv2==2.6.2) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from scikit-learn->nnunetv2==2.6.2) (1.5.1)\n",
      "Requirement already satisfied: PyYAML in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from yacs->nnunetv2==2.6.2) (6.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.6.2) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2==2.6.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->nnunetv2==2.6.2) (3.0.2)\n",
      "Requirement already satisfied: safetensors in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.5.3)\n",
      "Requirement already satisfied: torchvision in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.22.0)\n",
      "Requirement already satisfied: huggingface_hub in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (0.33.2)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting traceback2\n",
      "  Using cached traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/env/lib/python3.10/site-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2==2.6.2) (1.1.5)\n",
      "Collecting linecache2\n",
      "  Using cached linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Using legacy 'setup.py install' for acvl-utils, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for batchgenerators, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for dynamic-network-architectures, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: nnunetv2, batchgeneratorsv2\n",
      "  Building editable for nnunetv2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nnunetv2: filename=nnunetv2-2.6.2-0.editable-py3-none-any.whl size=16871 sha256=84c9dd84aa4b5c8997af4f09753c7c110605fae461e87ea672632a4d9b9cbfb8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-71h2blma/wheels/c9/d8/1b/cd9255a05863323ac62c7526c73fe60bcef63407315049005f\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.3.0-py3-none-any.whl size=65215 sha256=e274f7c710778760ec8e50840f7fb6127f5440675c0386be26576b674725ad38\n",
      "  Stored in directory: /home/abdal/.cache/pip/wheels/85/91/18/f246cf6cecb275d0e293271390794a0c090621cfc1ab1501c7\n",
      "Successfully built nnunetv2 batchgeneratorsv2\n",
      "Installing collected packages: SimpleITK, py-cpuinfo, linecache2, argparse, yacs, traceback2, tifffile, numexpr, ndindex, msgpack, lazy-loader, importlib-resources, imagecodecs, graphviz, future, connected-components-3d, unittest2, scikit-image, nibabel, blosc2, batchgenerators, fft-conv-pytorch, acvl-utils, timm, batchgeneratorsv2, dynamic-network-architectures, nnunetv2\n",
      "  Running setup.py install for batchgenerators ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for acvl-utils ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for dynamic-network-architectures ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed SimpleITK-2.5.2 acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.3.0 blosc2-3.10.2 connected-components-3d-3.26.0 dynamic-network-architectures-0.4.2 fft-conv-pytorch-1.2.0 future-1.0.0 graphviz-0.21 imagecodecs-2025.3.30 importlib-resources-6.5.2 lazy-loader-0.4 linecache2-1.0.0 msgpack-1.1.2 ndindex-1.10.0 nibabel-5.3.2 nnunetv2-2.6.2 numexpr-2.14.1 py-cpuinfo-9.0.0 scikit-image-0.25.2 tifffile-2025.5.10 timm-1.0.21 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/installation_instructions.md\n",
    "# Run only once!\n",
    "# !git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
    "# !cd nnUNet; pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd773bd-fa20-4f4c-b9db-89267fe83b31",
   "metadata": {},
   "source": [
    "#### nnunetv2 for 3D data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9efba-1a0e-4c25-b636-560184de4484",
   "metadata": {},
   "source": [
    "Dataset formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42aeeef8-29ab-4e72-a7be-26ece4106079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md\n",
    "# https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md\n",
    "base_dir = os.getcwd()\n",
    "data_path = os.path.join(base_dir, 'data')\n",
    "nnUNet_raw = os.path.join(data_path, 'nnUNet_raw')\n",
    "\n",
    "# create nnunet_raw dir\n",
    "os.makedirs(nnUNet_raw, exist_ok=True)\n",
    "\n",
    "# IBSR18 (3D) dataset for brain tissue segmentation wil be used as an example\n",
    "# TODO: download the dataset into the data folder\n",
    "ibsr18_path = os.path.join(data_path, 'IBSR18')\n",
    "\n",
    "# nnUNet_raw: This is where you place the raw datasets. This folder will have one subfolder for each dataset names DatasetXXX_YYY \n",
    "# where XXX is a 3-digit identifier (such as 001, 002, 043, 999, ...) and YYY is the (unique) dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e32537e8-dd3c-4e56-8959-ba04d5b25360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new dataset folder with nnunet folder naming inside the nnUnet_raw folder\n",
    "os.makedirs(os.path.join(nnUNet_raw, 'Dataset001_IBSR18'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f93a6bf-8261-4de4-9f13-a9ddf2c6b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuring the files inside the dataset folder\n",
    "# Note that this dataset has a single input channel (one modality type, for example, one of FLAIR, T1w, T1gd or T2w), thus we use _0000\n",
    "\n",
    "# Create new directory structure\n",
    "os.makedirs(os.path.join(nnUNet_raw, 'Dataset001_IBSR18', 'imagesTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(nnUNet_raw, 'Dataset001_IBSR18', 'imagesTs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(nnUNet_raw, 'Dataset001_IBSR18', 'labelsTr'), exist_ok=True)\n",
    "\n",
    "# Function to handle the file copying and renaming\n",
    "def handle_files(source_folder, dest_path, is_test=False):\n",
    "    for folder_name in sorted(os.listdir(source_folder)):\n",
    "        folder_path = os.path.join(source_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.nii.gz'):\n",
    "                    source_file = os.path.join(folder_path, file_name)\n",
    "                    if '_seg' in file_name:\n",
    "                        # For segmentation files (labels)\n",
    "                        dest_file = os.path.join(dest_path, 'labelsTr', folder_name + '.nii.gz')\n",
    "                    else:\n",
    "                        # For image files\n",
    "                        suffix = '_0000.nii.gz'\n",
    "                        dest_file = os.path.join(dest_path, 'imagesTs' if is_test else 'imagesTr', folder_name + suffix)\n",
    "                    shutil.copy2(source_file, dest_file)\n",
    "\n",
    "# Process each set\n",
    "# Here, 'Dataset001_IBSR18' is the newly named folder\n",
    "handle_files(source_folder=os.path.join(ibsr18_path, 'Training_Set'), dest_path=os.path.join(nnUNet_raw, 'Dataset001_IBSR18'))\n",
    "handle_files(source_folder=os.path.join(ibsr18_path, 'Validation_Set'), dest_path=os.path.join(nnUNet_raw, 'Dataset001_IBSR18'))\n",
    "handle_files(source_folder=os.path.join(ibsr18_path, 'Test_Set'), dest_path=os.path.join(nnUNet_raw, 'Dataset001_IBSR18'), is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d93f2-27aa-40da-a03b-796ff3327ef5",
   "metadata": {},
   "source": [
    "From the nnUNet git repo: 'For each training case, all images must have the same geometry to ensure that their pixel arrays are aligned. Also make sure that all your data is co-registered!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e399c454-b8e9-46ec-ba21-6431b5d4f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset JSON created successfully\n"
     ]
    }
   ],
   "source": [
    "# Creating the required json file\n",
    "def create_dataset_json(parent_dir):\n",
    "    # Define the structure of the JSON file\n",
    "    dataset_json = {\n",
    "        \"channel_names\": {\"0\": \"T1\"},\n",
    "        \"labels\": {\n",
    "            \"background\": 0,\n",
    "            \"CFS\": 1,\n",
    "            \"GM\": 2,\n",
    "            \"WM\": 3\n",
    "         }, \n",
    "        \"numTraining\": 0,\n",
    "        # \"numTest\": 0,\n",
    "        # \"training\": [],\n",
    "        # \"test\": [],\n",
    "        \"file_ending\": \".nii.gz\"\n",
    "    }\n",
    "\n",
    "    # Paths for training and test data\n",
    "    training_images_path = os.path.join(parent_dir, \"imagesTr\")\n",
    "    training_labels_path = os.path.join(parent_dir, \"labelsTr\")\n",
    "    test_images_path = os.path.join(parent_dir, \"imagesTs\")\n",
    "\n",
    "    # Scan for training images and labels\n",
    "    if os.path.exists(training_images_path) and os.path.exists(training_labels_path):\n",
    "        # training_images = sorted([f for f in os.listdir(training_images_path) if f.endswith('.nii.gz')])\n",
    "        training_labels = sorted([f for f in os.listdir(training_labels_path) if f.endswith('.nii.gz')])\n",
    "        # for img in training_labels:\n",
    "        #     dataset_json[\"training\"].append({\n",
    "        #             \"image\": os.path.join(\"./imagesTr\", img),\n",
    "        #             \"label\": os.path.join(\"./labelsTr\", img)\n",
    "        #         })\n",
    "        \n",
    "        dataset_json[\"numTraining\"] = len(training_labels)\n",
    "\n",
    "    # Scan for test images\n",
    "    # if os.path.exists(test_images_path):\n",
    "    #     test_images = sorted([f for f in os.listdir(test_images_path) if f.endswith('.nii.gz')])\n",
    "    #     for img in test_images:\n",
    "    #         dataset_json[\"test\"].append(os.path.join(\"./imagesTs\", img))\n",
    "        \n",
    "    #     dataset_json[\"numTest\"] = len(dataset_json[\"test\"])\n",
    "\n",
    "    # Write to JSON file\n",
    "    try:\n",
    "        with open(os.path.join(parent_dir, 'dataset.json'), 'w') as outfile:\n",
    "            json.dump(dataset_json, outfile, indent=4)\n",
    "            \n",
    "        print(f\"Dataset JSON created successfully\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error creating JSON file: {e}\")\n",
    "\n",
    "create_dataset_json(os.path.join(nnUNet_raw, 'Dataset001_IBSR18'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b137a70-a53b-44d3-bc06-f85fdd39a926",
   "metadata": {},
   "source": [
    "Experiment planning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27f844f6-c275-405c-9ffc-e381d81a1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnUNet_raw = ... # already created before\n",
    "nnUNet_preprocessed = os.path.join(data_path,'nnUNet_preprocessed')\n",
    "results_folder = os.path.join(data_path,'nnUNet_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3928ee99-e206-4543-9c0c-e09f7fd21581",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"nnUNet_raw\"] = str(nnUNet_raw)\n",
    "os.environ[\"nnUNet_preprocessed\"] = str(nnUNet_preprocessed)\n",
    "os.environ[\"nnUNet_results\"] = str(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f47c6c17-89cf-41a8-a200-a0e8a726c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset001_IBSR18\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "100%|███████████████████████████████████████████| 15/15 [00:30<00:00,  2.05s/it]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [115. 139. 147.], 3d_lowres: [115, 139, 147]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 69, 'patch_size': (np.int64(160), np.int64(160)), 'median_image_size_in_voxels': array([139., 147.]), 'spacing': array([0.9375, 0.9375]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(112), np.int64(128), np.int64(160)), 'median_image_size_in_voxels': array([115., 139., 147.]), 'spacing': array([1.5   , 0.9375, 0.9375]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to /mnt/c/Users/abdal/Documents/Projects/ai-notebooks/data/nnUNet_preprocessed/Dataset001_IBSR18/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset001_IBSR18\n",
      "Configuration: 2d...\n",
      "100%|███████████████████████████████████████████| 15/15 [00:53<00:00,  3.56s/it]\n",
      "Configuration: 3d_fullres...\n",
      "100%|███████████████████████████████████████████| 15/15 [00:38<00:00,  2.57s/it]\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset001_IBSR18. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# DATASET_ID=001 following the previous naming convension\n",
    "!nnUNetv2_plan_and_preprocess -d 001 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e68b71-65f9-4d3b-9810-24f76f9bb3f0",
   "metadata": {},
   "source": [
    "Model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e60d338-b7b6-457a-8c43-7beba62745a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nnUNetv2_train [-h] [-tr TR] [-p P]\n",
      "                      [-pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                      [-num_gpus NUM_GPUS] [--npz] [--c] [--val] [--val_best]\n",
      "                      [--disable_checkpointing] [-device DEVICE]\n",
      "                      dataset_name_or_id configuration fold\n",
      "\n",
      "positional arguments:\n",
      "  dataset_name_or_id    Dataset name or ID to train with\n",
      "  configuration         Configuration that should be trained\n",
      "  fold                  Fold of the 5-fold cross-validation. Should be an int\n",
      "                        between 0 and 4.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -tr TR                [OPTIONAL] Use this flag to specify a custom trainer.\n",
      "                        Default: nnUNetTrainer\n",
      "  -p P                  [OPTIONAL] Use this flag to specify a custom plans\n",
      "                        identifier. Default: nnUNetPlans\n",
      "  -pretrained_weights PRETRAINED_WEIGHTS\n",
      "                        [OPTIONAL] path to nnU-Net checkpoint file to be used\n",
      "                        as pretrained model. Will only be used when actually\n",
      "                        training. Beta. Use with caution.\n",
      "  -num_gpus NUM_GPUS    Specify the number of GPUs to use for training\n",
      "  --npz                 [OPTIONAL] Save softmax predictions from final\n",
      "                        validation as npz files (in addition to predicted\n",
      "                        segmentations). Needed for finding the best ensemble.\n",
      "  --c                   [OPTIONAL] Continue training from latest checkpoint\n",
      "  --val                 [OPTIONAL] Set this flag to only run the validation.\n",
      "                        Requires training to have finished.\n",
      "  --val_best            [OPTIONAL] If set, the validation will be performed\n",
      "                        with the checkpoint_best instead of checkpoint_final.\n",
      "                        NOT COMPATIBLE with --disable_checkpointing! WARNING:\n",
      "                        This will use the same 'validation' folder as the\n",
      "                        regular validation with no way of distinguishing the\n",
      "                        two!\n",
      "  --disable_checkpointing\n",
      "                        [OPTIONAL] Set this flag to disable checkpointing.\n",
      "                        Ideal for testing things out and you dont want to\n",
      "                        flood your hard drive with checkpoints.\n",
      "  -device DEVICE        Use this to set the device the training should run\n",
      "                        with. Available options are 'cuda' (GPU), 'cpu' (CPU)\n",
      "                        and 'mps' (Apple M1/M2). Do NOT use this to set which\n",
      "                        GPU ID! Use CUDA_VISIBLE_DEVICES=X nnUNetv2_train\n",
      "                        [...] instead!\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
