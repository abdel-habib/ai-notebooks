{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc9966f-9745-4454-8bd7-69ebf8005975",
   "metadata": {},
   "source": [
    "Every neural network layer, architecture, or model in PyTorch is built as a subclass of `torch.nn.Module` class. Here we implement and explore in depth different sub-topics related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7d087a-472a-463f-b66c-274f87748639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dda61-6975-44a5-adc1-fd0a3a845f27",
   "metadata": {},
   "source": [
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14289729-4d43-45f6-a775-e40bf4dca7c2",
   "metadata": {},
   "source": [
    "How different are nn.Parameters from torch.tensor()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e17cf-2a70-41c7-b098-51deb79a71d4",
   "metadata": {},
   "source": [
    "#### Composing Multiple Layers using Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600618e-d4af-42a7-9823-0d7e961c9317",
   "metadata": {},
   "source": [
    "It is a container module that stores other nn.Module layers in the order they are passed and executes them sequentially in the forward() pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71cd589-796d-49b6-b11f-70737eb11010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Number of layers: 2\n"
     ]
    }
   ],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU()\n",
    ")\n",
    "print(block)\n",
    "\n",
    "# we can nest nn.Sequential blocks inside other modules or inside other Sequentials\n",
    "model = nn.Sequential(\n",
    "    block,\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "print(model)\n",
    "print(\"Number of layers: \" + str(len(model)))          # Number of layers\n",
    "\n",
    "# by default, layers are given names based with int index\n",
    "# print(model[1])\n",
    "\n",
    "# for name, module in model.named_children():\n",
    "#     print(f\"Layer Name: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d83554-3f78-40c6-b77d-c3aaaf05af67",
   "metadata": {},
   "source": [
    "We can also use named layers with Sequential container, using `OrderedDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc81051-b0c8-4686-87c3-4995065e3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing the whole named model: \n",
      "Sequential(\n",
      "  (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "\n",
      "Accessing a single model layer: \n",
      "Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"conv\", nn.Conv2d(3, 8, 3, padding=1)),\n",
    "    (\"relu\", nn.ReLU()),\n",
    "    (\"pool\", nn.MaxPool2d(2))\n",
    "]))\n",
    "\n",
    "print(f\"Accessing the whole named model: \\n{model}\")\n",
    "print(f\"\\nAccessing a single model layer: \\n{model.conv}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a155ba-1d1a-4098-899d-96cdd072b0b7",
   "metadata": {},
   "source": [
    "#### Custom Modules (Decomposing Models into Blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb7dc4-7d6b-4421-9a9c-c18f78a547f3",
   "metadata": {},
   "source": [
    "We can create reusable blocks by subclassing `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db90cda-234c-4943-a9c7-57cd933967a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp1 = MLPBlock(10, 64)\n",
    "        self.mlp2 = MLPBlock(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = self.mlp2(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5162af28-6e90-4b11-bcc1-5faf41ccbf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FullModel                                [1, 1]                    --\n",
       "├─MLPBlock: 1-1                          [1, 64]                   --\n",
       "│    └─Sequential: 2-1                   [1, 64]                   --\n",
       "│    │    └─Linear: 3-1                  [1, 64]                   704\n",
       "│    │    └─BatchNorm1d: 3-2             [1, 64]                   128\n",
       "│    │    └─ReLU: 3-3                    [1, 64]                   --\n",
       "├─MLPBlock: 1-2                          [1, 32]                   --\n",
       "│    └─Sequential: 2-2                   [1, 32]                   --\n",
       "│    │    └─Linear: 3-4                  [1, 32]                   2,080\n",
       "│    │    └─BatchNorm1d: 3-5             [1, 32]                   64\n",
       "│    │    └─ReLU: 3-6                    [1, 32]                   --\n",
       "├─Linear: 1-3                            [1, 1]                    33\n",
       "==========================================================================================\n",
       "Total params: 3,009\n",
       "Trainable params: 3,009\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullModel()\n",
    "summary(model, input_size=(1, 10)) # b = 1, N features = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c7670-c567-430b-9ee5-0351d377622e",
   "metadata": {},
   "source": [
    "#### Modules with Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57063e0-d8e6-4978-ac09-833776023f81",
   "metadata": {},
   "source": [
    "This flexibility is essential for RNNs, dynamic routing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00c4e9a-290c-4432-a07b-ac7a3eba6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.mean() > 0:\n",
    "            return x + self.linear(x)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda4fb4-bcc4-4143-a9b7-ac58d4c727f2",
   "metadata": {},
   "source": [
    "#### Nested Modules (Using `ModuleList` and `ModuleDict`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df773da2-fb3f-4875-860a-34c7410a7301",
   "metadata": {},
   "source": [
    "We can create a list of layers using `ModuleList`. `ModuleList` is container to store layers, which are registered as submodules, but we must manually define the forward logic, unlike the previous `nn.Sequential` that didn't require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1062ffc-10b8-4d84-8a44-3a28d4c0de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "DeepMLP                                  [1, 1]                    --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Linear: 2-1                       [1, 64]                   704\n",
      "│    └─Linear: 2-2                       [1, 64]                   4,160\n",
      "│    └─Linear: 2-3                       [1, 64]                   4,160\n",
      "│    └─Linear: 2-4                       [1, 64]                   4,160\n",
      "│    └─Linear: 2-5                       [1, 64]                   4,160\n",
      "├─Linear: 1-2                            [1, 1]                    65\n",
      "==========================================================================================\n",
      "Total params: 17,409\n",
      "Trainable params: 17,409\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.02\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.07\n",
      "==========================================================================================\n",
      "DeepMLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1-4): 4 x Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, num_layers=5, in_dim=10, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.output(x)\n",
    "\n",
    "model = DeepMLP()\n",
    "print(summary(model, input_size=(1, 10))) # b = 1, N features = 10\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4611b30-393e-49fc-b596-f9ae2e0ba058",
   "metadata": {},
   "source": [
    "`ModuleDict` is a dictionary-like container for layers or blocks, keyed by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eb9001b-e754-4bda-b923-212815e10abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20])\n",
      "torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "class CustomBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleDict({\n",
    "            'a': nn.Linear(10, 20),\n",
    "            'b': nn.Linear(10, 30)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, branch='a'):\n",
    "        return self.branches[branch](x)\n",
    "\n",
    "model = CustomBranch()\n",
    "\n",
    "x = torch.randn(5, 10)  # batch of 5\n",
    "\n",
    "out_a = model(x, branch='a')  # uses nn.Linear(10, 20)\n",
    "out_b = model(x, branch='b')  # uses nn.Linear(10, 30)\n",
    "\n",
    "print(out_a.shape)  # torch.Size([5, 20])\n",
    "print(out_b.shape)  # torch.Size([5, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cae300-f47c-455d-b6fe-856093278608",
   "metadata": {},
   "source": [
    "ModuleDict is great when:\n",
    "* You have multiple branches or modules (e.g. for conditional computation) or you want named modules, accessed dynamically.\n",
    "* You're building things like: Mixture of Experts, task-specific heads in multitask learning, router-based architectures, or transformers with named components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
