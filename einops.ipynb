{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9788a505-d21e-4524-a94b-88f37b990531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeeab8a-c922-4d45-8ab3-c3d8e489afb7",
   "metadata": {},
   "source": [
    "<em>ops(x:Tensor, 'old -> new')</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b07a9-e264-44cf-8f1b-c5c8e703122d",
   "metadata": {},
   "source": [
    "Re-arranging elements according to a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415db46c-ccf3-4ba0-ad02-47910d0217e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8691, -0.6780,  0.1807,  1.5269],\n",
      "        [-2.2843,  0.0381,  0.0679, -0.2106],\n",
      "        [ 0.6618,  3.1145, -0.4201, -0.2379]])\n",
      "\n",
      "tensor([[-0.8691, -2.2843,  0.6618],\n",
      "        [-0.6780,  0.0381,  3.1145],\n",
      "        [ 0.1807,  0.0679, -0.4201],\n",
      "        [ 1.5269, -0.2106, -0.2379]])\n",
      "\n",
      "Before: torch.Size([2, 3, 4])\n",
      "After: torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)  # (batch, height, width)\n",
    "print(x[0], end=\"\\n\\n\")   # printing first batch \n",
    "\n",
    "y = rearrange(x, 'b h w -> b w h')  # Swap height and width\n",
    "print(y[0], end=\"\\n\\n\")\n",
    "\n",
    "print(\"Before:\", x.shape)\n",
    "print(\"After:\", y.shape)  # (2, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d44e0b-887d-444d-b5cf-82efb35aee23",
   "metadata": {},
   "source": [
    "For a larger tensor with `(n, h, w, p, q, c)` dimensions.\n",
    "\n",
    "* n: batch size - Number of samples in the batch (sometimes b)\n",
    "* h: height - Vertical spatial dimension (e.g., image height or feature map rows)\n",
    "* w: width - Horizontal spatial dimension (e.g., image width or feature map columns)\n",
    "* p: patch height or grid row - Often used for patch size or subdivisions of `h`\n",
    "* q: patch width or grid column - Often used for patch size or subdivisions of w\n",
    "* c:: channels - Number of channels (e.g., RGB = 3, or feature channels in CNNs)\n",
    "\n",
    "For example: (8, 4, 4, 4, 4, 3) → batch of 8 images, split into 4x4 grids of 4x4 patches, with 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32566483-26e1-4b1e-a6d1-20e700787046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 100, 200])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.RandomState(42).normal(size=[10, 32, 100, 200])\n",
    "\n",
    "x = torch.from_numpy(x)\n",
    "x.requires_grad = True\n",
    "\n",
    "x.shape # b c h w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8fe5a-3f92-4c1c-9df4-3d4a3db0127a",
   "metadata": {},
   "source": [
    "For `tensor.permute` dimension indices <br><br>\n",
    "Index - Dimension <br>\n",
    "0 - b (batch size) <br>\n",
    "1 - c (channels) <br>\n",
    "2 - h (height) <br>\n",
    "3 - w (width) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c931318-f899-4425-9fc6-dcc661053fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 200, 32]) torch.Size([10, 100, 200, 32])\n"
     ]
    }
   ],
   "source": [
    "# converting bchw to bhwc format and back is a common operation in CV\n",
    "y_einops = rearrange(x, 'b c h w -> b h w c') # Using einops.rearrange\n",
    "y_torch = x.permute(0, 2, 3, 1) # Using PyTorch's permute\n",
    "\n",
    "print(y_einops.shape, y_torch.shape) \n",
    "assert torch.equal(y_einops, y_torch), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7c2c833-ff90-424c-b5b8-ba9321cdca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1285.4242, dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "tensor(320., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y0 = x                                    # torch.Size([10, 32, 100, 200])\n",
    "y1 = reduce(y0, \"b c h w -> b c\", \"max\")  # torch.Size([10, 32]) - Apply global max pooling over spatial dimensions.\n",
    "y2 = rearrange(y1, \"b c -> c b\")          # torch.Size([32, 10])\n",
    "y3 = reduce(y2, \"c b -> \", \"sum\")         # torch.Size([]) - Sums all values to produce a single scalar.\n",
    "print(y3)\n",
    "\n",
    "y3.backward()\n",
    "print(reduce(x.grad, \"b c h w -> \", \"sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a011e-6ec2-4d49-b8cc-836bd03c61a4",
   "metadata": {},
   "source": [
    "<b>Flattening</b> is common operation, frequently appears at the boundary between convolutional layers and fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42c18d7d-6626-4334-b400-c0d090d634fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 640000]) torch.Size([10, 640000]) torch.Size([10, 640000]) torch.Size([10, 640000])\n"
     ]
    }
   ],
   "source": [
    "# 4D tensor [b, c, h, w] to a 2D tensor [b, c*h*w]\n",
    "y_einops = rearrange(x, \"b c h w -> b (c h w)\")\n",
    "\n",
    "# view() reshapes a tensor without copying memory (efficient).\n",
    "# x.size(0) gives the batch size b.\n",
    "# -1 tells PyTorch to infer the correct size for the second dimension (i.e., c * h * w).\n",
    "y_torch0 = x.view(x.size(0), -1) \n",
    "\n",
    "# .reshape() is similar to .view(), but automatically makes a copy if needed.\n",
    "# Can be slightly slower than .view() if it ends up copying memory.\n",
    "y_torch1 = x.reshape(x.shape[0], -1)\n",
    "\n",
    "# Flattens all dimensions starting from start_dim=1 (i.e., flattens [c, h, w]).\n",
    "# Very clean and readable for flattening \"everything after the batch\".\n",
    "y_torch2 = x.flatten(start_dim=1)\n",
    "\n",
    "print(y_einops.shape, y_torch0.shape, y_torch1.shape, y_torch2.shape) \n",
    "assert torch.equal(y_einops, y_torch0), \"Not identical operation\"\n",
    "assert torch.equal(y_einops, y_torch1), \"Not identical operation\"\n",
    "assert torch.equal(y_einops, y_torch2), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3acae4-8030-49fc-85a2-803dbdcacd23",
   "metadata": {},
   "source": [
    "<b>space-to-depth</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e31478b1-492d-4678-b265-1ed446bc5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Rearranges spatial/temporal blocks into the channel dimension.\n",
    "# Used for downsampling without losing information (e.g. in real-time detection).\n",
    "\n",
    "y_einpos = rearrange(x, \"b c (h h1) (w w1) -> b (h1 w1 c) h w\", h1=2, w1=2)\n",
    "print(y_einops.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607a086-db8c-4823-9510-a9d6e0cc711f",
   "metadata": {},
   "source": [
    "<b>depth-to-space</b> (notice that it's reverse of the previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32747e8a-ec2c-47f4-9d1e-df3e11dacfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "y_einpos = rearrange(x, \"b (h1 w1 c) h w -> b c (h h1) (w w1)\", h1=2, w1=2)\n",
    "print(y_einops.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f390a6d-25c5-4217-9c87-8c1a14eaa443",
   "metadata": {},
   "source": [
    "Simple <b>global average pooling</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6deb1d78-dc6a-4d51-966f-ba9ebe1e99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32]) torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "# Averages each channel over the spatial dimensions (h, w).\n",
    "# Equivalent to Global Average Pooling (GAP), common in CNN classification heads\n",
    "y_einops = reduce(x, \"b c h w -> b c\", reduction=\"mean\")\n",
    "\n",
    "y_torch0 = x.mean((2,3)) # reduce over height and width\n",
    "\n",
    "print(y_einops.shape, y_torch0.shape)\n",
    "assert torch.equal(y_einops, y_torch0), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc776bc-a75a-4ccd-a776-87e4cd381f9f",
   "metadata": {},
   "source": [
    "<b>max-pooling</b> with a kernel 2x2 (2D) - reduce over spatial (H×W)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2e7ccce-c681-4e97-a2f4-6134fd6b3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 50, 100]) torch.Size([10, 32, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "# Treats (h h1) and (w w1) as 2×2 blocks and takes max over each block.\n",
    "# Equivalent to 2D MaxPooling with kernel_size=2 and stride=2.\n",
    "y_einops = reduce(x,  \"b c (h h1) (w w1) -> b c h w\", reduction=\"max\", h1=2, w1=2)\n",
    "# same as the above, a 2×2 max pooling, with shorthand for specifying factor sizes directly.\n",
    "# y_einops = reduce(x, \"b c (h 2) (w 2) -> b c h w\", reduction=\"max\") \n",
    "\n",
    "y_torch0 = F.max_pool2d(x, kernel_size=2, stride=2) # 2d max pooling\n",
    "\n",
    "print(y_einops.shape, y_torch0.shape)\n",
    "assert torch.equal(y_einops, y_torch0), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239af2f7-3755-4cfb-9236-ba6f434c5de9",
   "metadata": {},
   "source": [
    "<b>Temporal Max-Pooling (Batched Sequences)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab9df8ba-5ec3-4af5-a6a4-a5c54bd56055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 4]) torch.Size([4, 2, 4]) torch.Size([4, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# for sequential 1-d models, you'll probably want pooling over time\n",
    "# it applies temporal max-pooling over time dimension.\n",
    "# it also assumes time has been reshaped as (t*2) — combines pairs of time steps and reduces to max.\n",
    "\n",
    "# einops example\n",
    "# reduce(x, '(t 2) b c -> t b c', reduction='max')\n",
    "\n",
    "# pytorch equivalent\n",
    "# t2, b, c = x.shape\n",
    "# x = x.view(t2 // 2, 2, b, c)  # reshape to [t, 2, b, c]\n",
    "# y = x.max(dim=1).values      # max over the 2-timestep window\n",
    "\n",
    "# Temporal Max-Pooling (Batched Sequences) are used in sequence models (audio, video, nlp) by applying max operation \n",
    "# over time intervals (e.g. every 2 time steps)\n",
    "# useful for downsampling temporal resolution (reducing length while keeping important features)\n",
    "\n",
    "## using different x input example than the global one\n",
    "x_t = torch.randn(8, 2, 4)  # (t*2, b, c) → (4*2, 2, 4)\n",
    "y_t_einops = reduce(x_t, '(t 2) b c -> t b c', reduction='max')\n",
    "\n",
    "x_reshaped = x_t.view(4, 2, 2, 4)  # [t, 2, b, c]\n",
    "y_t_torch = x_reshaped.max(dim=1).values  # max over 2-timestep window\n",
    "\n",
    "print(x_t.shape, y_t_einops.shape, y_t_torch.shape)\n",
    "assert torch.equal(y_t_einops, y_t_torch), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b5c2b-3287-44b3-870a-efb83299f4a6",
   "metadata": {},
   "source": [
    "**3D Max-Pooling** - Reduce over volumetric blocks\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "797b391d-58fc-463a-8a50-3691b40f4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 8, 8]) torch.Size([1, 3, 4, 4, 4]) torch.Size([1, 3, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# for volumetric models, all three dimensions are pooled\n",
    "# 3D max-pooling over non-overlapping 2x2x2 cubes (depth, height, width).\n",
    "# Common in 3D vision (e.g., volumetric data, medical imaging).\n",
    "\n",
    "# einops example\n",
    "# reduce(x, 'b c (x 2) (y 2) (z 2) -> b c x y z', reduction='max')\n",
    "\n",
    "# pytorch example\n",
    "# y = F.max_pool3d(x, kernel_size=2, stride=2)\n",
    "\n",
    "x_3d = torch.randn(1, 3, 8, 8, 8) # [B, C, D, H, W]\n",
    "y_3d_einops = reduce(x_3d, 'b c (d 2) (h 2) (w 2) -> b c d h w', reduction='max')\n",
    "y_3d_torch  = F.max_pool3d(x_3d, kernel_size=2, stride=2) \n",
    "\n",
    "print(x_3d.shape, y_3d_einops.shape, y_3d_torch.shape)\n",
    "assert torch.equal(y_3d_einops, y_3d_torch), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b517c-4209-4f4d-9eb0-41abd452ceea",
   "metadata": {},
   "source": [
    "**Squeeze and unsqueeze (expand_dims)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96de0683-5cc3-4105-b30f-b1bfc60fa417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 200, 3])\n",
      "torch.Size([1, 3, 100, 200])\n",
      "torch.Size([1, 60000])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# models typically work only with batches,\n",
    "# so to predict a single image ...\n",
    "\n",
    "# took 1 image from the batch and re-arranged the axis\n",
    "image = rearrange(x[0, :3], \"c h w -> h w c\") \n",
    "print(image.shape)\n",
    "\n",
    "# ... create a dummy 1-element axis ...\n",
    "y_einops = rearrange(image, \"h w c -> () c h w\")\n",
    "print(y_einops.shape)\n",
    "\n",
    "# ... imagine you predicted this with a convolutional network for classification,\n",
    "# we'll just flatten axes ...\n",
    "predictions = rearrange(y, \"b c h w -> b (c h w)\")\n",
    "print(predictions.shape)\n",
    "\n",
    "# ... finally, decompose (remove) dummy axis\n",
    "predictions = rearrange(predictions, \"() classes -> classes\")\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994884c-fc51-4bd1-8d89-788dd3d02a62",
   "metadata": {},
   "source": [
    "**keepdims-like behavior for reductions** <br>\n",
    "* empty composition () provides dimensions of length 1, which are broadcastable. <br>\n",
    "* alternatively, you can use just 1 to introduce new axis, that's a synonym to ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c6433f9-f351-496a-a7ba-6833a6d22db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 100, 200]) torch.Size([10, 32, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, reduce(x, \"b c h w -> b c 1 1\", \"mean\").shape)\n",
    "assert torch.equal(reduce(x, \"b c h w -> b c 1 1\", \"mean\"), reduce(x, \"b c h w -> b c () ()\", \"mean\")), \"Not identical operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "41815d24-f616-4642-924b-d224588a536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 100, 200])\n",
      "torch.Size([10, 32, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "# per-channel mean-normalization for each image:\n",
    "y_einops = x - reduce(x, \"b c h w -> b c 1 1\", \"mean\")\n",
    "print(y_einops.shape)\n",
    "\n",
    "# per-channel mean-normalization for whole batch:\n",
    "y_einops = x - reduce(x, \"b c h w -> 1 c 1 1\", \"mean\")\n",
    "print(y_einops.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee946c-1689-4f2f-927c-2e3d3b281bad",
   "metadata": {},
   "source": [
    "**Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f90620f8-6c98-4c8b-9e9e-c9e6d6542691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "list_of_tensors = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1dc0b06f-b301-493c-98e5-5885bd5f34d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 200, 32])\n"
     ]
    }
   ],
   "source": [
    "# New axis (one that enumerates tensors) appears first on the left side of expression. Just as if we were indexing list \n",
    "#first we'd get tensor by index\n",
    "tensors = rearrange(list_of_tensors, \"b c h w -> b h w c\")\n",
    "print(tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aec9bd2e-4be3-4c13-a108-8943201f82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 200, 32, 10])\n"
     ]
    }
   ],
   "source": [
    "# or maybe stack along last dimension?\n",
    "tensors = rearrange(list_of_tensors, \"b c h w -> h w c b\")\n",
    "print(tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94e518-4509-4d0f-a983-c0a0753ba376",
   "metadata": {},
   "source": [
    "**Concatenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb0860e7-2eb3-4cb5-8e02-15dc7063cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 200, 32])\n"
     ]
    }
   ],
   "source": [
    "# concatenate over the first dimension?\n",
    "tensors = rearrange(list_of_tensors, \"b c h w -> (b h) w c\")\n",
    "print(tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c816c6a7-e04e-4098-a0fa-bdc8422d65d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 200, 320])\n"
     ]
    }
   ],
   "source": [
    "# or maybe concatenate along last dimension?\n",
    "tensors = rearrange(list_of_tensors, \"b c h w -> h w (b c)\")\n",
    "print(tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052afa9-594f-4bfd-b6d4-a3a7a3e864db",
   "metadata": {},
   "source": [
    "**Shuffling within a dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "221add43-78b2-4448-9f9b-9b611f1d2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 100, 200])\n",
      "torch.Size([10, 32, 100, 200])\n"
     ]
    }
   ],
   "source": [
    "# channel shuffle (as it is drawn in shufflenet paper)\n",
    "y_einops = rearrange(x, \"b (g1 g2 c) h w-> b (g2 g1 c) h w\", g1=4, g2=4)\n",
    "print(y_einops.shape)\n",
    "\n",
    "# simpler version of channel shuffle\n",
    "y_einops = rearrange(x, \"b (g c) h w-> b (c g) h w\", g=4)\n",
    "print(y_einops.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
