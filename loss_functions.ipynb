{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deac9a8b-9f78-4d34-992f-311c728c3b65",
   "metadata": {},
   "source": [
    "Experimenting and re-implementing different loss functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29848de-d894-4206-9540-558674408a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf56133-d125-4670-b5a7-fdc51f63d66e",
   "metadata": {},
   "source": [
    "#### Binary Cross-Entropy / Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77704c-1072-4199-9a6c-94dd2b641f4e",
   "metadata": {},
   "source": [
    "Binary cross-entropy (log loss) is a loss function used in **binary classification problems**.  It quantifies the difference between the actual class labels (0 or 1) and the predicted probabilities output by the model. The lower the binary cross-entropy value, the better the model’s predictions align with the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6d6d7-56ed-4d96-a95e-8e0d22fcd33b",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy measures the distance between the true labels and the predicted probabilities. When the predicted probability is close to the actual label, the BCE value is low, indicating a good prediction. Conversely, when the predicted probability deviates significantly from the actual label, the BCE value is high, indicating a poor prediction. The logarithmic component of the BCE function penalizes wrong predictions more heavily than correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "797bdd8f-d093-4e86-9edd-ecf13054e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([0, 1, 1, 1], dtype=torch.float32)\n",
    "y_pred = torch.tensor([0.1, 0.9, 0.8, 0.3], dtype=torch.float32) # model’s output is a probability between 0 and 1\n",
    "\n",
    "y_true_np = y_true.numpy()\n",
    "y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb6bfacb-75c6-4bf5-b4cf-2faf4c662629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_true, y_pred):   \n",
    "    eps = 1e-9\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "    return -np.mean(y_true*np.log(y_pred) + (1 - y_true)*np.log(1 - y_pred))\n",
    "\n",
    "loss = F.binary_cross_entropy(y_pred, y_true)\n",
    "loss_bce = bce(y_true_np, y_pred_np)\n",
    "\n",
    "assert np.allclose(loss, loss_bce), \"Incorrect implementation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc45d61-5a01-483f-8e4b-7418fae95af3",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE) / L2 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321a13f-4cfc-48d7-990a-f2b36fd211e6",
   "metadata": {},
   "source": [
    "Quantifies the magnitude of the error between an algorithm prediction and an actual output by taking the average of the squared difference between the predictions and the target values. It is useful for **regression tasks**, particularly when we want to penalize larger errors more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5be21d28-fd6f-4e1d-885d-2b238f958af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred): \n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "loss_mse = mse(y_true_np, y_pred_np)\n",
    "\n",
    "assert np.allclose(loss, loss_mse), \"Incorrect implementation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed8c90-2aa2-4f3f-82fb-1cf739ba1ffd",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE) / L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360128fc-c58d-40cb-b05e-d9fc9c89c8d4",
   "metadata": {},
   "source": [
    "Used in **regression tasks** that calculates the average absolute differences between predicted values from a machine learning model and the actual target values. Unlike Mean Squared Error (MSE), MAE does not square the differences, treating all errors with equal weight regardless of their magnitude. Compared to MSE, MAE does not square the differences, which makes it less sensitive to outliers because it assigns an equal weight to all errors, regardless of their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cac6c08-ac9a-4f47-8761-9feb785c8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred): \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "loss = F.l1_loss(y_pred, y_true)\n",
    "loss_mae = mae(y_true_np, y_pred_np)\n",
    "\n",
    "assert np.allclose(loss, loss_mae), \"Incorrect implementation\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
