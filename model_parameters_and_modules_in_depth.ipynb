{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc9966f-9745-4454-8bd7-69ebf8005975",
   "metadata": {},
   "source": [
    "Every neural network layer, architecture, or model in PyTorch is built as a subclass of `torch.nn.Module` class. Here we implement and explore in depth different sub-topics related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7d087a-472a-463f-b66c-274f87748639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574463a-be01-47be-a0d6-d18c591ed2ed",
   "metadata": {},
   "source": [
    "#### Pytorch nn.Parameter and nn.Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dda61-6975-44a5-adc1-fd0a3a845f27",
   "metadata": {},
   "source": [
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when theyâ€™re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82afd459-0b7e-4340-b908-2f260fd53f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: weight, Values: Parameter containing:\n",
      "tensor([[ 0.3147, -0.3086,  0.0777, -0.6696],\n",
      "        [-1.5764,  0.8109,  0.4712,  0.7366],\n",
      "        [-0.5918,  0.1557, -0.8665, -0.0236]], requires_grad=True), Requires Grad: True\n",
      "Parameter: bias, Values: Parameter containing:\n",
      "tensor([-2.4605, -0.4729, -1.6643, -0.1562], requires_grad=True), Requires Grad: True\n",
      "\n",
      "input: tensor([-0.4777, -2.4996, -2.7967]).\n",
      "output: tensor([ 2.9846, -2.7880, -0.4557, -1.6114], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Here, the state consists of randomly-initialized weight and bias tensors that define the affine transformation. \n",
    "# Because each of these is defined as a Parameter, they are registered for the module and will automatically be tracked and \n",
    "# returned from calls to parameters()\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (input @ self.weight) + self.bias\n",
    "\n",
    "linear_layer = LinearLayer(3, 4)\n",
    "\n",
    "# it is a generator object\n",
    "# print(linear_layer.parameters())\n",
    "\n",
    "# note that the parameters are given the names of the class declared parameter variables\n",
    "for name, param in linear_layer.named_parameters():\n",
    "    print(f\"Parameter: {name}, Values: {param}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "x_input = torch.randn(3)\n",
    "y_output = linear_layer(x_input)\n",
    "print(f\"\\ninput: {x_input}.\\noutput: {y_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14289729-4d43-45f6-a775-e40bf4dca7c2",
   "metadata": {},
   "source": [
    "How different are nn.Parameters from torch.tensor()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6e1f4a7-4ba9-4d55-ac6d-782e19bc8fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The parameter below\n",
    "param1 = nn.Parameter(torch.randn(3, 3)) # torch.Size([3, 3]), requires_grad=True as default\n",
    "print(param1.requires_grad)\n",
    "\n",
    "# is similar to ...\n",
    "tensor = torch.randn(3, 3, requires_grad= True) # Not a parameter!\n",
    "param2 = nn.Parameter(tensor)\n",
    "print(param2.requires_grad)\n",
    "\n",
    "# but tensor is not a parameter by it's own, meaning that it won't be found in the list of model.parameters()\n",
    "# thus, tensor is not registered as a trainable parameter\n",
    "\n",
    "# When a tensor is wrapped with nn.Parameter and assigned as an attribute of an nn.Module, \n",
    "# it's automatically added to the list returned by model.parameters() and model.named_parameters(), similar to the\n",
    "# example in the previous cell. The optimizer (e.g., torch.optim.Adam) will also update nn.Parameters during training.\n",
    "# Therefor, nn.Parameter allows you to introduce weights that are learnable.\n",
    "\n",
    "# Note, to freeze a parameter, we can use param.requires_grad = False\n",
    "# It will stay in model.parameters() but will not receive gradient updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e17cf-2a70-41c7-b098-51deb79a71d4",
   "metadata": {},
   "source": [
    "#### Composing Multiple Layers using Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600618e-d4af-42a7-9823-0d7e961c9317",
   "metadata": {},
   "source": [
    "It is a container module that stores other nn.Module layers in the order they are passed and executes them sequentially in the forward() pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d71cd589-796d-49b6-b11f-70737eb11010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Number of layers: 2\n"
     ]
    }
   ],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU()\n",
    ")\n",
    "print(block)\n",
    "\n",
    "# we can nest nn.Sequential blocks inside other modules or inside other Sequentials\n",
    "model = nn.Sequential(\n",
    "    block,\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "print(model)\n",
    "print(\"Number of layers: \" + str(len(model)))          # Number of layers\n",
    "\n",
    "# by default, layers are given names based with int index\n",
    "# print(model[1])\n",
    "\n",
    "# for name, module in model.named_children():\n",
    "#     print(f\"Layer Name: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d83554-3f78-40c6-b77d-c3aaaf05af67",
   "metadata": {},
   "source": [
    "We can also use named layers with Sequential container, using `OrderedDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cc81051-b0c8-4686-87c3-4995065e3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing the whole named model: \n",
      "Sequential(\n",
      "  (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "\n",
      "Accessing a single model layer: \n",
      "Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "Layer Name: conv, Module: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Layer Name: relu, Module: ReLU()\n",
      "Layer Name: pool, Module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    (\"conv\", nn.Conv2d(3, 8, 3, padding=1)),\n",
    "    (\"relu\", nn.ReLU()),\n",
    "    (\"pool\", nn.MaxPool2d(2))\n",
    "]))\n",
    "\n",
    "print(f\"Accessing the whole named model: \\n{model}\")\n",
    "print(f\"\\nAccessing a single model layer: \\n{model.conv}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a155ba-1d1a-4098-899d-96cdd072b0b7",
   "metadata": {},
   "source": [
    "#### Custom Modules (Decomposing Models into Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db90cda-234c-4943-a9c7-57cd933967a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
