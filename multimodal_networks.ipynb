{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc5b259-1cfe-4510-8a0f-5e0a26601f37",
   "metadata": {},
   "source": [
    "Multi-modal networks usually referred to architectures that can handle multiple inputs simultaneously, such as combining images and text, or processing multiple feature sets. This notebooks simplifies the concept with some simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9005c7-9505-41b5-b059-bf127c2205c2",
   "metadata": {},
   "source": [
    "These networks usually take two or more different inputs (possibly of different types or shapes) and combine them somewhere in the architecture. They’re widely used in:\n",
    "* Vision + Language tasks (e.g., image captioning, visual question answering)\n",
    "* Tabular + Text or Image + Metadata fusion\n",
    "* Siamese or Triplet Networks for similarity comparison\n",
    "* Multi-modal learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81d20c8-3f6e-412f-b41a-93c1ab985736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f94182-8bfd-420d-b421-dc095825f5c8",
   "metadata": {},
   "source": [
    "#### Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f30428-5e20-43ae-97a7-248c301a50c2",
   "metadata": {},
   "source": [
    "<b>Example 1: Tabular + Image Input</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984232ca-9006-4937-8465-d315b44ab0d0",
   "metadata": {},
   "source": [
    "Such networks takes both an image and a tabular feature vector (e.g., age, temperature, metadata) as an input, then fuses them to perform a certain task (i.e. classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d4b61-e81b-477f-ba6b-af3dc1178934",
   "metadata": {},
   "source": [
    "```text\n",
    "[Image] -----> [CNN] ---------\\\n",
    "                               +--> [Fusion] --> [FC] --> [Output]\n",
    "[Tabular] --> [Dense Layer] --/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82eded7-b375-43ba-887c-7002b9cd1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTabularNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Image branch/encoder (simple CNN)\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Tabular branch/encoder\n",
    "        self.tabular_branch = nn.Sequential(\n",
    "            nn.Linear(5, 32),  # Assume 5 tabular features\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion + classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 8 * 8 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3)  # 3 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular):\n",
    "        image_feat = self.cnn_branch(image)\n",
    "        tabular_feat = self.tabular_branch(tabular)\n",
    "        combined = torch.cat((image_feat, tabular_feat), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31b266-189b-4cb0-80f5-ea3d062c65a3",
   "metadata": {},
   "source": [
    "In the above network, the input shapes is `[batch_size, 3, 64, 64]` for image and `[batch_size, 5]` for tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45d363-b579-4a90-9d04-12e0712cd3e1",
   "metadata": {},
   "source": [
    "<b>Example 2: Text + Image Input (e.g., CLIP-style)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa537e-ad7a-497a-8bc0-7002dd94d89a",
   "metadata": {},
   "source": [
    "This pattern encodes an image and a text input separately and compares their embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01097d9-8a93-4cea-ab16-10d0b29622c0",
   "metadata": {},
   "source": [
    "```text\n",
    "[Image] -----> [CNN/ViT] -----------\\\n",
    "                                     +--> [Similarity / Logits]\n",
    "[Text] -----> [LSTM/BERT] ----------/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78562886-9fc8-43c0-b6f9-88aa5897414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A close implementation to clip architecture\n",
    "class ImageTextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ...\n",
    "        self.text_encoder = ...\n",
    "\n",
    "    def encode_images(self, batch):\n",
    "        return self.image_encoder(torch.flatten(batch, 1))\n",
    "\n",
    "    def encode_text(self, batch):\n",
    "        return self.text_encoder(batch['text_tokens'])\n",
    "\n",
    "    def forward(self, batch, **kwargs):\n",
    "        '''Forward pass of the model training.'''\n",
    "        # extract the features\n",
    "        image_features = self.encode_images(batch)                      # [n, image_features_dimension]\n",
    "        text_features  = self.encode_text(batch, text_pooling='eos')    # [n, model_output_dimension]\n",
    "\n",
    "        # linear projection to map from each encoder’s representation to the multi-modal embedding space.\n",
    "        image_embeddings = self.image_projection_layer(image_features) if self.image_projection_layer is not None else image_features  # [n, output_projection_dimension]\n",
    "        text_embeddings  = self.text_projection_layer(text_features)   if self.text_projection_layer  is not None else text_features  # [n, output_projection_dimension]\n",
    "\n",
    "        # normalise the embeddings\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True) # [n, output_projection_dimension]\n",
    "        text_embeddings  = text_embeddings  / text_embeddings.norm(dim=1, keepdim=True)  # [n, output_projection_dimension]\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp() # clamp the logit_scale?\n",
    "\n",
    "        logits_per_image = logit_scale * image_embeddings @ text_embeddings.t() # [n, n]\n",
    "        logits_per_text  = logit_scale * text_embeddings @ image_embeddings.t() # [n, n]\n",
    "\n",
    "        output = {\n",
    "            \"image_embeddings\": image_embeddings,\n",
    "            \"text_embeddings\": text_embeddings,\n",
    "            \"logit_scale\": logit_scale,\n",
    "            \"logits_per_image\": logits_per_image,\n",
    "            \"logits_per_text\": logits_per_text\n",
    "        }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe9c7c-87d4-4762-89d6-45d74592bf5b",
   "metadata": {},
   "source": [
    "<b>Example 3: Siamese Network with 2 Inputs</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a894769-b5c5-452c-a627-55fb38a2b704",
   "metadata": {},
   "source": [
    "Two inputs of the same kind, passed through shared weights to compute similarity (e.g., face verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52373f2d-50c2-4f08-8799-a246a9d8afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        emb1 = self.encoder(input1)\n",
    "        emb2 = self.encoder(input2)\n",
    "        # Compute L1 or cosine similarity\n",
    "        return F.pairwise_distance(emb1, emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a09c4-cb60-4083-9697-869e19cf920c",
   "metadata": {},
   "source": [
    "<b>Example 4: Multi-Input with nn.ModuleDict or ModuleList</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781bc05-8a00-4d0b-af53-daa808c7e384",
   "metadata": {},
   "source": [
    "We can use `nn.ModuleDict` to dynamically define input branches based on input types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70c449f-86cf-4591-94b0-bc4926767e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleDict({\n",
    "            'img': nn.Sequential(nn.Conv2d(1, 8, 3), nn.Flatten()),\n",
    "            'text': nn.Sequential(nn.Linear(300, 128), nn.ReLU()),\n",
    "            'meta': nn.Sequential(nn.Linear(10, 32))\n",
    "        })\n",
    "        self.classifier = nn.Linear(8*26*26 + 128 + 32, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: dict with keys 'img', 'text', 'meta'\n",
    "        features = [self.branches[k](v) for k, v in inputs.items()]\n",
    "        combined = torch.cat(features, dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1b670-ed8b-4ba5-93e9-14f7bfb50bce",
   "metadata": {},
   "source": [
    "#### Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc71c3d-b8ae-43e1-914b-a03178a1f2d9",
   "metadata": {},
   "source": [
    "We'll usually need to modify the training loop and dataloader to load both modalities. For example, in the code below, we load a sample of image and tabular data from for a single batch iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465a615-092c-48ee-8e62-5fab2ef64d43",
   "metadata": {},
   "source": [
    "```python\n",
    "for batch in dataloader:\n",
    "    img = batch['image'].to(device)\n",
    "    tab = batch['tabular'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    outputs = model(img, tab)\n",
    "    loss = criterion(outputs, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9488ea-a212-4266-9c34-62755d276644",
   "metadata": {},
   "source": [
    "Both modalities data are passed to the model, which will go through the `forward` function of the model class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
