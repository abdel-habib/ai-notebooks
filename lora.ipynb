{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e21a28-e4bc-45b9-a827-813ea66fa042",
   "metadata": {},
   "source": [
    "Implementation of `LorA: Low-Rank Adaptation of Large Language Models` paper in pytorch with a short touch-up on matrix theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0981712e-796d-4856-8539-c9ec6a5663ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTImageProcessor \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d615c-92ac-40d7-a771-285424d07c03",
   "metadata": {},
   "source": [
    "#### Matrix Rank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7d3f5-29eb-48ff-91d0-11a338ef6d6d",
   "metadata": {},
   "source": [
    "Any matrix $A$ of rank k can be decomposed into a long and skinny matrix times a short and long one.\n"
   ]
  },
  {
   "attachments": {
    "9e88e3b3-ccca-4d0b-b91d-47b9ed3bc412.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "780ac77e-3dc8-42a7-9fb8-9c49349d1ff4",
   "metadata": {},
   "source": [
    "![image.png](attachment:9e88e3b3-ccca-4d0b-b91d-47b9ed3bc412.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f87f3e-f1fd-4018-ad97-3883db019ced",
   "metadata": {},
   "source": [
    "The general definition of matrix ank is as following: a matrix $A$ has rank $k$ if it can be written as te sum of two rank-1 matrices. In other words, $A$ can be written as the product of a long and skinny $(m \\times k)$ matrix $Y$ and a short and long $(k \\times n)$ matrix $Z^{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd9565-633a-4921-b051-5cbf81e73db0",
   "metadata": {},
   "source": [
    "In the below example, Row 3 = Row 1 + Row 2 ‚áí not independent. Only Row 1 and Row 2 are independent ‚áí rank = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "49ee0187-e62e-4e66-8f1a-618c93f373cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [5, 7, 9]   # This row = row1 + row2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "torch.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67185cd9-6e1f-4b5a-8523-8a7cd1181dd3",
   "metadata": {},
   "source": [
    "All rows are linearly independent ‚áí rank = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "853e9110-542a-4ae0-a17a-de113232e070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 0, 6],\n",
    "    [7, 8, 9]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "torch.linalg.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002795f-6929-4ea7-9a2f-458f23c40ddf",
   "metadata": {},
   "source": [
    "Each row is a multiple of [1, 2] ‚Üí same direction ‚Üí rank = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "24e13d1f-968b-4b0b-8e52-5eebec110602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([\n",
    "    [2, 4],\n",
    "    [1, 2],\n",
    "    [3, 6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "torch.linalg.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c88d5e-ba32-4b71-92b0-3020f5096537",
   "metadata": {},
   "source": [
    "No information, just zeros ‚áí rank = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d7690f23-454f-4802-8349-6877a0d0dd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.zeros((4, 4))\n",
    "torch.linalg.matrix_rank(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0766d4-cb25-454c-a47d-6bbdaf7a2995",
   "metadata": {},
   "source": [
    "#### Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e32c5-2e79-4fa8-a3ec-0c32e05c6c8a",
   "metadata": {},
   "source": [
    "Matrix decomposition is the process of breaking a matrix into simpler matrices (often to expose structure or reduce computation). Such process approximates a given matrix $A$ with a rank-$k$ matrix, for a target rank $k$. Such a matrix is called a low-rank approximation. A low-rank approximation provides a (lossy) compressed version of the matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298204f-6613-4a3b-9403-0c14c5645e9e",
   "metadata": {},
   "source": [
    "Methods (just the types):\n",
    "1. Signular Value Decomposition (SVD)\n",
    "2. Low-Rank Approximation from SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c868391-a26f-4496-b8aa-0fdbca209f2a",
   "metadata": {},
   "source": [
    "#### LoRA Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed994c95-2239-49a8-8f90-e2bce41e217e",
   "metadata": {},
   "source": [
    "As mentioned in the paper, \"LoRA allows us to train some dense layers in a neural network indirctly by optimizing rank decomposition matrices of the dense layers' change during adapttion instead, while keeping the pre-trained weights frozen\", as shown in the illustration below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e934307-03dc-4ccc-a142-7cea63e0fb31",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"><img src=\"./public/lora.png\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53dd24-8be1-445e-910e-ac80244eeaf8",
   "metadata": {},
   "source": [
    "LoRA possess several advantages, summarised from the paper:\n",
    "* A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
    "ferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\n",
    "matrices A and B in the Figure above, reducing the storage requirement and task-switching over-\n",
    "head signiÔ¨Åcantly.\n",
    "* LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\n",
    "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
    "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
    "much smaller low-rank matrices.\n",
    "* The simple linear design allows us to merge the trainable matrices with the frozen weights\n",
    "when deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\n",
    "construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53976b0-8e33-4f53-9b46-b0ba231a796e",
   "metadata": {},
   "source": [
    "Therefore, LoRA freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer of the transformer. This makes it possible to efficiently fine-tune large langauge models by reducing trainable parameters by a large factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72357e-0d00-4766-b57e-189421f065c2",
   "metadata": {},
   "source": [
    "LoRA linear layer adds a low-rank decomposition to the pre-trained weight matrix $W \\in \\Bbb R^{d \\times k} $ of the linear layer. It proposes freezing the original weights and injecting low-rank update matrices into each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84c666-a49f-45d0-ab12-bed0d8cf3407",
   "metadata": {},
   "source": [
    "| Symbol      | Meaning | Description |\n",
    "| ----- | ---------------------- | -------------------------------------------- |\n",
    "| $W_{0}$      | Weight Matrix          | Initial pre-trained weight matrix $\\in \\Bbb R^{d \\times k}$.      |\n",
    "| $k$      | Input Dimension          | The number of features going into the layer.      |\n",
    "| $d$      | Output  Dimension        | The number of features coming out of the layer.  |\n",
    "| $r$      | LoRA rank (compression dim) | The intrinsic dimension used to approximate the weight update. It controls the size of the low-rank update matrix $\\Delta W = BA$.  |\n",
    "| $A$      | Down projection        | $r \\times k$  |\n",
    "| $B$      | Up projection        | $d \\times r$ |\n",
    "| $x$      | Input vector        | The vector $\\in \\Bbb R^{k}$, however, it is passed to the layer as batch of shape $(n, k)$ |\n",
    "| $BAx$      | Low-rank update        | $\\in \\Bbb R^{d}$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa554f-5423-40f4-a6bb-53f94f510400",
   "metadata": {},
   "source": [
    "If a layer's weight is $W_{0} \\in \\Bbb R^{d \\times k} $, LoRA represents the weight update as $W_{0} + \\Delta W = W_{0} + BA $, where:\n",
    "* $B  \\in  \\Bbb R^{d \\times r}$\n",
    "* $A  \\in  \\Bbb R^{r \\times k}$\n",
    "* Both $B$ and $A$ have much smaller inner dimension $r << min(d, k)$.\n",
    "\n",
    "During training, $W_{0}$ stays frozen, and only A,B are learned. The forward pass througgh this adapted layer is them $h = W_{0}x + (BA)x$, often scaled by a factor $\\frac{\\alpha}{r}$. for stability. This effectively adds a small \"change\" matrix BA to the base layer's output without modifying $W_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d583d8-c952-42f9-b3c2-6a5aed3ddddb",
   "metadata": {},
   "source": [
    "Standard `Linear` dense layers with $W_{0} \\in \\Bbb R^{d \\times k} $ and input $x \\in \\Bbb R^{k} $ computes $W_{0}x + bias$. LoRA views this weight matrix as full-rank (rank $min(ùëë,ùëò)$) in general. And instead of fine-tunning $W_{0}$, it adds two smaller matrices, $B$ and $A$ whose product $BA$ is rank-$r$. Both $B$ and $A$ are multiplied by the same input $x$ (first $Ax \\in \\Bbb R^{r}$, then $B(Ax) \\in \\Bbb R^{d}$) and summed with $W_{0}x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e32c1-ee4d-4a5a-b9fd-b7609d08fd49",
   "metadata": {},
   "source": [
    "By choosing $r$ small (even 1-4 for large layers), the number of trainable parameters drops dramatically, yet $W_{0} + BA$ still has a dimension $d x k$ and affects the layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4327192-a2d7-4b07-a8c6-3f0af12da92c",
   "metadata": {},
   "source": [
    "#### LoRA Linear Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b17c0b4c-2353-47ba-865d-e631a002fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Notes:\n",
    "        # x shape         : (batch_size, k)\n",
    "        # A.T shape       : (k, r)\n",
    "        # x @ A.T         : (batch_size, r)     == Ax\n",
    "        # B.T shape       : (r, d_out)\n",
    "        # (Ax) @ B.T      : (batch_size, d_out) == B(Ax)\n",
    "\n",
    "        \n",
    "        # Set Œ±=r is not provided \n",
    "        # i.e. make the scaling factor alpha/r =1 as initially set alpha to the first r and we do not tune it.\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "\n",
    "        # Initial Weight Frozen\n",
    "        self.weight = nn.Parameter(torch.empty(size=(out_features, in_features))) # W0\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features)) # or torch.empty((out_features,))\n",
    "            self.bias.requires_grad = False\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # scaling delta W by alpha/r as in the paper\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_a = nn.Parameter(torch.empty(size=(r, in_features)))\n",
    "        self.lora_b = nn.Parameter(torch.empty(size=(out_features, r)))\n",
    "\n",
    "        # From the paper: \n",
    "        # \"We use a random Gaussian initialization for A and zero for B, so ‚àÜW = BA is zero at the beginning of training\"\n",
    "        with torch.no_grad():\n",
    "            nn.init.kaiming_uniform_(self.lora_a, a=5 ** 0.5)\n",
    "            nn.init.zeros_(self.lora_b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output = nn.functional.linear(x, self.weight, bias=self.bias) # W0\n",
    "        output += (x @ self.lora_a.T @ self.lora_b.T) * self.scaling\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "473a0974-b3b1-4ef9-822b-4470606dccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LoRALinear(\n",
    "    in_features = 10, # k\n",
    "    out_features = 10, # d\n",
    "    bias = True, \n",
    "    r = 4,\n",
    "    alpha = None\n",
    ")\n",
    "\n",
    "x = torch.ones((8, 10)) # n, k\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5552d4f2-2d13-4d4e-b796-894efcd24d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "assert x.shape == y.shape, \"Shapes not matching\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9190-7173-4924-a26e-e2efbd0e5dba",
   "metadata": {},
   "source": [
    "Only `lora_a` and `lora_b` must be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "63a99c3a-8b28-4e5c-80b7-c71d5ac4b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-4.3908e+06,  4.5783e-41, -6.5029e+22,  3.0910e-41, -2.0746e+06,\n",
       "            4.5783e-41, -2.2348e-16,  4.5782e-41, -2.2348e-16,  4.5782e-41],\n",
       "          [-2.2348e-16,  4.5782e-41, -1.9009e+06,  4.5783e-41, -2.2348e-16,\n",
       "            4.5782e-41,  1.1339e+22,  4.5782e-41, -1.7726e+06,  4.5783e-41],\n",
       "          [-1.9427e+06,  4.5783e-41, -1.7726e+06,  4.5783e-41,  1.1339e+22,\n",
       "            4.5782e-41, -4.0910e-22,  4.5783e-41, -1.9427e+06,  4.5783e-41],\n",
       "          [ 1.1339e+22,  4.5782e-41, -4.0910e-22,  4.5783e-41, -1.9427e+06,\n",
       "            4.5783e-41,  1.1339e+22,  4.5782e-41, -2.1381e+06,  4.5783e-41],\n",
       "          [-1.9426e+06,  4.5783e-41, -1.7726e+06,  4.5783e-41, -2.1381e+06,\n",
       "            4.5783e-41, -2.1381e+06,  4.5783e-41,  1.1339e+22,  4.5782e-41],\n",
       "          [-2.2348e-16,  4.5782e-41, -1.9427e+06,  4.5783e-41, -6.4976e+04,\n",
       "            4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9428e+06,  4.5783e-41],\n",
       "          [-1.9009e+06,  4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9009e+06,\n",
       "            4.5783e-41, -6.4976e+04,  4.5783e-41, -1.9009e+06,  4.5783e-41],\n",
       "          [-1.9428e+06,  4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9009e+06,\n",
       "            4.5783e-41,  1.1339e+22,  4.5782e-41, -8.3594e+04,  4.5783e-41],\n",
       "          [-1.9426e+06,  4.5783e-41, -8.3594e+04,  4.5783e-41, -8.3594e+04,\n",
       "            4.5783e-41, -8.3594e+04,  4.5783e-41,  1.1339e+22,  4.5782e-41],\n",
       "          [-1.9428e+06,  4.5783e-41, -2.9406e-16,  4.5782e-41, -2.2568e+06,\n",
       "            4.5783e-41, -1.9426e+06,  4.5783e-41, -2.9406e-16,  4.5782e-41]])),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.3452e-43,  0.0000e+00,  8.9683e-44,  0.0000e+00, -3.6318e-34,\n",
       "           3.0917e-41,  7.3604e+22,  4.1878e-11,  7.2150e+22,  7.2251e+28])),\n",
       " ('lora_a',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1772,  0.0408,  0.2112,  0.1531,  0.0105,  0.2881,  0.1873, -0.0319,\n",
       "           -0.1415, -0.0082],\n",
       "          [-0.0987,  0.2028, -0.3151,  0.2860,  0.2336, -0.0068, -0.2626,  0.1924,\n",
       "            0.2174,  0.2253],\n",
       "          [ 0.0782, -0.2005, -0.0107, -0.1138,  0.2794, -0.1678, -0.2930,  0.2028,\n",
       "           -0.2527,  0.2582],\n",
       "          [ 0.2098, -0.1837, -0.1928,  0.2048, -0.2532,  0.2120, -0.0389, -0.0626,\n",
       "            0.1318, -0.2378]], requires_grad=True)),\n",
       " ('lora_b',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870abe4-6828-409b-a14e-a8a98cf3e0af",
   "metadata": {},
   "source": [
    "#### Testing LoRA implementation on a HuggingFace vision model and CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d2bdfe3e-e361-4123-a0e4-df1a745afa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9e501f0-1684-46e9-b569-af3cf5cbd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before LoRA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model before LoRA\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7e4db82e-2a3e-4ab8-9ad1-6fe34da932e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora_in_vit(model, r=8):\n",
    "    for block in model.vit.encoder.layer:\n",
    "        # grab original dense layers\n",
    "        d = block.attention.attention.query.in_features\n",
    "        # replace with LoRA versions for query and value\n",
    "        block.attention.attention.query = LoRALinear(d, d, bias=False, r=r)\n",
    "        block.attention.attention.value = LoRALinear(d, d, bias=False, r=r)\n",
    "\n",
    "inject_lora_in_vit(model, r=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074e2c2-1af7-4e1a-bfbd-3ef283280fd5",
   "metadata": {},
   "source": [
    "Injecting the LoRA can be seen in the model where the linear layers changed to the `LoRALinear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5e4732ec-e445-4517-97a7-bcf296806b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after LoRA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): LoRALinear()\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LoRALinear()\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model after LoRA\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a5ef9d59-e14e-46a7-b57e-62c6646a5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") # ViTFeatureExtractor will be depricated\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(feature_extractor.image_mean, feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "train_ds = CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_ds  = CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e6102c48-ecb3-43e0-8302-9045b165a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125 x 16 samples.\n",
      "625 x 16 samples.\n"
     ]
    }
   ],
   "source": [
    "print(str(len(train_loader)) + \" x 16 samples.\\n\" + str(len(test_loader))+ \" x 16 samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0cfbc9d8-131a-45db-875d-84b93b01d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    for xb, yb in tqdm(loader):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=xb).logits\n",
    "        loss = F.cross_entropy(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(pixel_values=xb).logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e760e40-08ac-4f65-8b0a-37ee0b92d696",
   "metadata": {},
   "source": [
    "Run both training cells below when large GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "516e7f00-01fd-4741-a53b-e75e923c82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_full = ViTForImageClassification.from_pretrained(\n",
    "#     \"google/vit-base-patch16-224-in21k\", num_labels=10\n",
    "# ).to(device)\n",
    "# opt_full = optim.Adam(model_full.parameters(), lr=3e-5)\n",
    "\n",
    "# train_one_epoch(model_full, train_loader, opt_full, device)\n",
    "# acc_full = evaluate(model_full, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0d344dfd-256e-4f1f-910a-9cff21c8000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lora = ViTForImageClassification.from_pretrained(\n",
    "#     \"google/vit-base-patch16-224-in21k\", num_labels=10\n",
    "# )\n",
    "# inject_lora_in_vit(model_lora, r=4)\n",
    "\n",
    "# # Freeze everything except LoRA\n",
    "# for name, param in model_lora.named_parameters():\n",
    "#     if \"lora_\" not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# model_lora = model_lora.to(device)\n",
    "# opt_lora = optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=3e-5)\n",
    "\n",
    "# train_one_epoch(model_lora, train_loader, opt_lora, device)\n",
    "# acc_lora = evaluate(model_lora, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
