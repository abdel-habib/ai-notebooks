{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e21a28-e4bc-45b9-a827-813ea66fa042",
   "metadata": {},
   "source": [
    "Implementation of `LorA: Low-Rank Adaptation of Large Language Models` paper in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed994c95-2239-49a8-8f90-e2bce41e217e",
   "metadata": {},
   "source": [
    "As mentioned in the paper, \"LoRA allows us to train some dense layers in a neural network indirctly by optimizing rank decomposition matrices of the dense layers' change during adapttion instead, while keeping the pre-trained weights frozen\", as shown in the illustration below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e934307-03dc-4ccc-a142-7cea63e0fb31",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"><img src=\"./public/lora.png\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53dd24-8be1-445e-910e-ac80244eeaf8",
   "metadata": {},
   "source": [
    "LoRA possess several advantages, summarised from the paper:\n",
    "* A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
    "ferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\n",
    "matrices A and B in the Figure above, reducing the storage requirement and task-switching over-\n",
    "head signiÔ¨Åcantly.\n",
    "* LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\n",
    "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
    "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
    "much smaller low-rank matrices.\n",
    "* The simple linear design allows us to merge the trainable matrices with the frozen weights\n",
    "when deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\n",
    "construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53976b0-8e33-4f53-9b46-b0ba231a796e",
   "metadata": {},
   "source": [
    "Therefor, LoRA freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer of the transformer. This makes it possible to efficiently fine-tune large langauge models by reducing trainable parameters by a large factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0981712e-796d-4856-8539-c9ec6a5663ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c868391-a26f-4496-b8aa-0fdbca209f2a",
   "metadata": {},
   "source": [
    "#### LoRA Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72357e-0d00-4766-b57e-189421f065c2",
   "metadata": {},
   "source": [
    "LoRA linear layer adds a low-rank decomposition to the pre-trained weight matrix $W \\in \\Bbb R^{d \\times k} $ of the linear layer. It proposes freezing the original weights and injecting low-rank update matrices into each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84c666-a49f-45d0-ab12-bed0d8cf3407",
   "metadata": {},
   "source": [
    "| Symbol      | Meaning | Description |\n",
    "| ----- | ---------------------- | -------------------------------------------- |\n",
    "| $k$      | Input Dimension          | The number of features going into the layer.      |\n",
    "| $d$      | Output  Dimension        | The number of features coming out of the layer.  |\n",
    "| $r$      | LoRA rank (compression dim) | The intrinsic dimension used to approximate the weight update. It controls the size of the low-rank update matrix $\\Delta W = BA$.  |\n",
    "| $A$      | Down projection        | $r \\times k$  |\n",
    "| $B$      | Up projection        | $d \\times r$ |\n",
    "| $x$      | Input vector        | The vector $\\in \\Bbb R^{k}$, however, it is passed to the layer as batch of shape $(n, k)$ |\n",
    "| $BAx$      | Low-rank update        | $\\in \\Bbb R^{d}$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa554f-5423-40f4-a6bb-53f94f510400",
   "metadata": {},
   "source": [
    "If a layer's weight is $W_{0} \\in \\Bbb R^{d \\times k} $, LoRA represents the weight update as $W_{0} + \\Delta W = W_{0} + BA $, where:\n",
    "* $B  \\in  \\Bbb R^{d \\times r}$\n",
    "* $A  \\in  \\Bbb R^{r \\times k}$\n",
    "* Both $B$ and $A$ have much smaller inner dimension $r << min(d, k)$.\n",
    "\n",
    "During training, $W_{0}$ stays frozen, and only A,B are learned. The forward pass througgh this adapted layer is them $h = W_{0}x + (BA)x$, often scaled by a factor $\\frac{\\alpha}{r}$. for stability. This effectively adds a small \"change\" matrix BA to the base layer's output without modifying $W_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d583d8-c952-42f9-b3c2-6a5aed3ddddb",
   "metadata": {},
   "source": [
    "Standard `Linear` dense layers with $W_{0} \\in \\Bbb R^{d \\times k} $ and input $x \\in \\Bbb R^{k} $ computes $W_{0}x + bias$. LoRA views this weight matrix as full-rank (rank $min(ùëë,ùëò)$) in general. And instead of fine-tunning $W_{0}$, it adds two smaller matrices, $B$ and $A$ whose product $BA$ is rank-$r$. Both $B$ and $A$ are multiplied by the same input $x$ (first $Ax \\in \\Bbb R^{r}$, then $B(Ax) \\in \\Bbb R^{d}$) and summed with $W_{0}x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e32c1-ee4d-4a5a-b9fd-b7609d08fd49",
   "metadata": {},
   "source": [
    "By choosing $r$ small (even 1-4 for large layers), the number of trainable parameters drops dramatically, yet $W_{0} + BA$ still has a dimension $d x k$ and affects the layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4327192-a2d7-4b07-a8c6-3f0af12da92c",
   "metadata": {},
   "source": [
    "#### LoRA Linear Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b17c0b4c-2353-47ba-865d-e631a002fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Notes:\n",
    "        # x shape         : (batch_size, k)\n",
    "        # A.T shape       : (k, r)\n",
    "        # x @ A.T         : (batch_size, r)     == Ax\n",
    "        # B.T shape       : (r, d_out)\n",
    "        # (Ax) @ B.T      : (batch_size, d_out) == B(Ax)\n",
    "\n",
    "        \n",
    "        # Set Œ±=r is not provided \n",
    "        # i.e. make the scaling factor alpha/r =1 as initially set alpha to the first r and we do not tune it.\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "\n",
    "        # Initial Weight Frozen\n",
    "        self.weight = nn.Parameter(torch.empty(size=(out_features, in_features))) # W0\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features)) # or torch.empty((out_features,))\n",
    "            self.bias.requires_grad = False\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # scaling delta W by alpha/r as in the paper\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_a = nn.Parameter(torch.empty(size=(r, in_features)))\n",
    "        self.lora_b = nn.Parameter(torch.empty(size=(out_features, r)))\n",
    "\n",
    "        # From the paper: \n",
    "        # \"We use a random Gaussian initialization for A and zero for B, so ‚àÜW = BA is zero at the beginning of training\"\n",
    "        with torch.no_grad():\n",
    "            nn.init.kaiming_uniform_(self.lora_a, a=5 ** 0.5)\n",
    "            nn.init.zeros_(self.lora_b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output = nn.functional.linear(x, self.weight, bias=self.bias) # W0\n",
    "        output += (x @ self.lora_a.T @ self.lora_b.T) * self.scaling\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "473a0974-b3b1-4ef9-822b-4470606dccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Linear(\n",
    "    in_features = 10, # k\n",
    "    out_features = 10, # d\n",
    "    bias = True, \n",
    "    r = 4,\n",
    "    alpha = None\n",
    ")\n",
    "\n",
    "x = torch.ones((8, 10)) # n, k\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5552d4f2-2d13-4d4e-b796-894efcd24d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "assert x.shape == y.shape, \"Shapes not matching\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9190-7173-4924-a26e-e2efbd0e5dba",
   "metadata": {},
   "source": [
    "Only `lora_a` and `lora_b` must be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "63a99c3a-8b28-4e5c-80b7-c71d5ac4b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-4.3908e+06,  4.5783e-41, -4.3908e+06,  4.5783e-41, -2.2521e-16,\n",
       "            4.5782e-41, -2.2518e-16,  4.5782e-41, -2.2526e-16,  4.5782e-41],\n",
       "          [-2.2512e-16,  4.5782e-41, -2.2381e-16,  4.5782e-41, -2.2346e-16,\n",
       "            4.5782e-41, -3.7146e-19,  4.5782e-41, -2.3118e-16,  4.5782e-41],\n",
       "          [-2.2222e-16,  4.5782e-41, -2.2413e-16,  4.5782e-41, -3.7177e-19,\n",
       "            4.5782e-41, -2.0052e-16,  4.5782e-41, -2.2522e-16,  4.5782e-41],\n",
       "          [-2.2514e-16,  4.5782e-41, -2.2540e-16,  4.5782e-41, -2.2527e-16,\n",
       "            4.5782e-41, -2.2529e-16,  4.5782e-41, -2.2538e-16,  4.5782e-41],\n",
       "          [-2.2549e-16,  4.5782e-41, -2.2544e-16,  4.5782e-41, -2.1103e-16,\n",
       "            4.5782e-41, -2.3273e-16,  4.5782e-41, -2.3274e-16,  4.5782e-41],\n",
       "          [-2.3273e-16,  4.5782e-41, -2.2394e-16,  4.5782e-41, -2.2395e-16,\n",
       "            4.5782e-41, -2.2393e-16,  4.5782e-41, -2.2338e-16,  4.5782e-41],\n",
       "          [ 3.5873e-43,  0.0000e+00,  3.8115e-43,  0.0000e+00, -2.6871e+20,\n",
       "            3.0910e-41,  2.9708e-43,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5783e-41,  1.8788e+31,\n",
       "            1.7220e+22,  2.1715e-18,  2.1781e-04,  1.3482e-05,  1.0606e-08],\n",
       "          [ 4.1958e-08,  4.4154e-05,  6.7204e-07,  3.4152e-06,  1.3150e+22,\n",
       "            1.0572e-05,  1.3201e+19,  2.4422e-18,  3.1360e+27,  7.0800e+31],\n",
       "          [ 3.1095e-18,  4.7851e+22,  2.8826e+32,  4.4248e+30,  7.6729e+34,\n",
       "            2.1707e-18,  4.5447e+30,  7.0062e+22,  2.1715e-18,  2.6331e+20]])),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-2.7404e+05,  4.5783e-41, -3.0673e-16,  4.5782e-41, -5.1068e+05,\n",
       "           4.5783e-41, -2.0746e+06,  4.5783e-41, -3.0677e-16,  4.5782e-41])),\n",
       " ('lora_a',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2112, -0.1343, -0.1370, -0.2540, -0.1336,  0.0165, -0.0967,  0.3018,\n",
       "            0.2074,  0.1897],\n",
       "          [-0.2157,  0.1048, -0.1616, -0.1467, -0.2952,  0.3048,  0.1351,  0.2552,\n",
       "            0.0844,  0.2030],\n",
       "          [-0.1461,  0.1067, -0.0928,  0.2067, -0.0011, -0.2366,  0.2331,  0.0753,\n",
       "           -0.2640,  0.0497],\n",
       "          [ 0.1641, -0.3046,  0.2206,  0.3138,  0.0846,  0.0562,  0.0480, -0.0252,\n",
       "            0.2493,  0.1176]], requires_grad=True)),\n",
       " ('lora_b',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer.named_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
