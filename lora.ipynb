{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e21a28-e4bc-45b9-a827-813ea66fa042",
   "metadata": {},
   "source": [
    "Implementation of `LorA: Low-Rank Adaptation of Large Language Models` paper in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed994c95-2239-49a8-8f90-e2bce41e217e",
   "metadata": {},
   "source": [
    "As mentioned in the paper, \"LoRA allows us to train some dense layers in a neural network indirctly by optimizing rank decomposition matrices of the dense layers' change during adapttion instead, while keeping the pre-trained weights frozen\", as shown in the illustration below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e934307-03dc-4ccc-a142-7cea63e0fb31",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"><img src=\"./public/lora.png\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53dd24-8be1-445e-910e-ac80244eeaf8",
   "metadata": {},
   "source": [
    "LoRA possess several advantages, summarised from the paper:\n",
    "* A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
    "ferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\n",
    "matrices A and B in the Figure above, reducing the storage requirement and task-switching over-\n",
    "head signiÔ¨Åcantly.\n",
    "* LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\n",
    "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
    "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
    "much smaller low-rank matrices.\n",
    "* The simple linear design allows us to merge the trainable matrices with the frozen weights\n",
    "when deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\n",
    "construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53976b0-8e33-4f53-9b46-b0ba231a796e",
   "metadata": {},
   "source": [
    "Therefor, LoRA freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer of the transformer. This makes it possible to efficiently fine-tune large langauge models by reducing trainable parameters by a large factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0981712e-796d-4856-8539-c9ec6a5663ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTImageProcessor \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c868391-a26f-4496-b8aa-0fdbca209f2a",
   "metadata": {},
   "source": [
    "#### LoRA Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72357e-0d00-4766-b57e-189421f065c2",
   "metadata": {},
   "source": [
    "LoRA linear layer adds a low-rank decomposition to the pre-trained weight matrix $W \\in \\Bbb R^{d \\times k} $ of the linear layer. It proposes freezing the original weights and injecting low-rank update matrices into each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84c666-a49f-45d0-ab12-bed0d8cf3407",
   "metadata": {},
   "source": [
    "| Symbol      | Meaning | Description |\n",
    "| ----- | ---------------------- | -------------------------------------------- |\n",
    "| $k$      | Input Dimension          | The number of features going into the layer.      |\n",
    "| $d$      | Output  Dimension        | The number of features coming out of the layer.  |\n",
    "| $r$      | LoRA rank (compression dim) | The intrinsic dimension used to approximate the weight update. It controls the size of the low-rank update matrix $\\Delta W = BA$.  |\n",
    "| $A$      | Down projection        | $r \\times k$  |\n",
    "| $B$      | Up projection        | $d \\times r$ |\n",
    "| $x$      | Input vector        | The vector $\\in \\Bbb R^{k}$, however, it is passed to the layer as batch of shape $(n, k)$ |\n",
    "| $BAx$      | Low-rank update        | $\\in \\Bbb R^{d}$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa554f-5423-40f4-a6bb-53f94f510400",
   "metadata": {},
   "source": [
    "If a layer's weight is $W_{0} \\in \\Bbb R^{d \\times k} $, LoRA represents the weight update as $W_{0} + \\Delta W = W_{0} + BA $, where:\n",
    "* $B  \\in  \\Bbb R^{d \\times r}$\n",
    "* $A  \\in  \\Bbb R^{r \\times k}$\n",
    "* Both $B$ and $A$ have much smaller inner dimension $r << min(d, k)$.\n",
    "\n",
    "During training, $W_{0}$ stays frozen, and only A,B are learned. The forward pass througgh this adapted layer is them $h = W_{0}x + (BA)x$, often scaled by a factor $\\frac{\\alpha}{r}$. for stability. This effectively adds a small \"change\" matrix BA to the base layer's output without modifying $W_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d583d8-c952-42f9-b3c2-6a5aed3ddddb",
   "metadata": {},
   "source": [
    "Standard `Linear` dense layers with $W_{0} \\in \\Bbb R^{d \\times k} $ and input $x \\in \\Bbb R^{k} $ computes $W_{0}x + bias$. LoRA views this weight matrix as full-rank (rank $min(ùëë,ùëò)$) in general. And instead of fine-tunning $W_{0}$, it adds two smaller matrices, $B$ and $A$ whose product $BA$ is rank-$r$. Both $B$ and $A$ are multiplied by the same input $x$ (first $Ax \\in \\Bbb R^{r}$, then $B(Ax) \\in \\Bbb R^{d}$) and summed with $W_{0}x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e32c1-ee4d-4a5a-b9fd-b7609d08fd49",
   "metadata": {},
   "source": [
    "By choosing $r$ small (even 1-4 for large layers), the number of trainable parameters drops dramatically, yet $W_{0} + BA$ still has a dimension $d x k$ and affects the layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4327192-a2d7-4b07-a8c6-3f0af12da92c",
   "metadata": {},
   "source": [
    "#### LoRA Linear Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b17c0b4c-2353-47ba-865d-e631a002fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Notes:\n",
    "        # x shape         : (batch_size, k)\n",
    "        # A.T shape       : (k, r)\n",
    "        # x @ A.T         : (batch_size, r)     == Ax\n",
    "        # B.T shape       : (r, d_out)\n",
    "        # (Ax) @ B.T      : (batch_size, d_out) == B(Ax)\n",
    "\n",
    "        \n",
    "        # Set Œ±=r is not provided \n",
    "        # i.e. make the scaling factor alpha/r =1 as initially set alpha to the first r and we do not tune it.\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "\n",
    "        # Initial Weight Frozen\n",
    "        self.weight = nn.Parameter(torch.empty(size=(out_features, in_features))) # W0\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features)) # or torch.empty((out_features,))\n",
    "            self.bias.requires_grad = False\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # scaling delta W by alpha/r as in the paper\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_a = nn.Parameter(torch.empty(size=(r, in_features)))\n",
    "        self.lora_b = nn.Parameter(torch.empty(size=(out_features, r)))\n",
    "\n",
    "        # From the paper: \n",
    "        # \"We use a random Gaussian initialization for A and zero for B, so ‚àÜW = BA is zero at the beginning of training\"\n",
    "        with torch.no_grad():\n",
    "            nn.init.kaiming_uniform_(self.lora_a, a=5 ** 0.5)\n",
    "            nn.init.zeros_(self.lora_b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output = nn.functional.linear(x, self.weight, bias=self.bias) # W0\n",
    "        output += (x @ self.lora_a.T @ self.lora_b.T) * self.scaling\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "473a0974-b3b1-4ef9-822b-4470606dccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LoRALinear(\n",
    "    in_features = 10, # k\n",
    "    out_features = 10, # d\n",
    "    bias = True, \n",
    "    r = 4,\n",
    "    alpha = None\n",
    ")\n",
    "\n",
    "x = torch.ones((8, 10)) # n, k\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5552d4f2-2d13-4d4e-b796-894efcd24d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "assert x.shape == y.shape, \"Shapes not matching\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9190-7173-4924-a26e-e2efbd0e5dba",
   "metadata": {},
   "source": [
    "Only `lora_a` and `lora_b` must be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "63a99c3a-8b28-4e5c-80b7-c71d5ac4b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-4.3908e+06,  4.5783e-41, -6.5029e+22,  3.0910e-41, -2.0746e+06,\n",
       "            4.5783e-41, -2.2348e-16,  4.5782e-41, -2.2348e-16,  4.5782e-41],\n",
       "          [-2.2348e-16,  4.5782e-41, -1.9009e+06,  4.5783e-41, -2.2348e-16,\n",
       "            4.5782e-41,  1.1339e+22,  4.5782e-41, -1.7726e+06,  4.5783e-41],\n",
       "          [-1.9427e+06,  4.5783e-41, -1.7726e+06,  4.5783e-41,  1.1339e+22,\n",
       "            4.5782e-41, -4.0910e-22,  4.5783e-41, -1.9427e+06,  4.5783e-41],\n",
       "          [ 1.1339e+22,  4.5782e-41, -4.0910e-22,  4.5783e-41, -1.9427e+06,\n",
       "            4.5783e-41,  1.1339e+22,  4.5782e-41, -2.1381e+06,  4.5783e-41],\n",
       "          [-1.9426e+06,  4.5783e-41, -1.7726e+06,  4.5783e-41, -2.1381e+06,\n",
       "            4.5783e-41, -2.1381e+06,  4.5783e-41,  1.1339e+22,  4.5782e-41],\n",
       "          [-2.2348e-16,  4.5782e-41, -1.9427e+06,  4.5783e-41, -6.4976e+04,\n",
       "            4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9428e+06,  4.5783e-41],\n",
       "          [-1.9009e+06,  4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9009e+06,\n",
       "            4.5783e-41, -6.4976e+04,  4.5783e-41, -1.9009e+06,  4.5783e-41],\n",
       "          [-1.9428e+06,  4.5783e-41, -1.9009e+06,  4.5783e-41, -1.9009e+06,\n",
       "            4.5783e-41,  1.1339e+22,  4.5782e-41, -8.3594e+04,  4.5783e-41],\n",
       "          [-1.9426e+06,  4.5783e-41, -8.3594e+04,  4.5783e-41, -8.3594e+04,\n",
       "            4.5783e-41, -8.3594e+04,  4.5783e-41,  1.1339e+22,  4.5782e-41],\n",
       "          [-1.9428e+06,  4.5783e-41, -2.9406e-16,  4.5782e-41, -2.2568e+06,\n",
       "            4.5783e-41, -1.9426e+06,  4.5783e-41, -2.9406e-16,  4.5782e-41]])),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.3452e-43,  0.0000e+00,  8.9683e-44,  0.0000e+00, -3.6318e-34,\n",
       "           3.0917e-41,  7.3604e+22,  4.1878e-11,  7.2150e+22,  7.2251e+28])),\n",
       " ('lora_a',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1772,  0.0408,  0.2112,  0.1531,  0.0105,  0.2881,  0.1873, -0.0319,\n",
       "           -0.1415, -0.0082],\n",
       "          [-0.0987,  0.2028, -0.3151,  0.2860,  0.2336, -0.0068, -0.2626,  0.1924,\n",
       "            0.2174,  0.2253],\n",
       "          [ 0.0782, -0.2005, -0.0107, -0.1138,  0.2794, -0.1678, -0.2930,  0.2028,\n",
       "           -0.2527,  0.2582],\n",
       "          [ 0.2098, -0.1837, -0.1928,  0.2048, -0.2532,  0.2120, -0.0389, -0.0626,\n",
       "            0.1318, -0.2378]], requires_grad=True)),\n",
       " ('lora_b',\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870abe4-6828-409b-a14e-a8a98cf3e0af",
   "metadata": {},
   "source": [
    "#### Testing LoRA implementation on a HuggingFace vision model and CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d2bdfe3e-e361-4123-a0e4-df1a745afa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9e501f0-1684-46e9-b569-af3cf5cbd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before LoRA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model before LoRA\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7e4db82e-2a3e-4ab8-9ad1-6fe34da932e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora_in_vit(model, r=8):\n",
    "    for block in model.vit.encoder.layer:\n",
    "        # grab original dense layers\n",
    "        d = block.attention.attention.query.in_features\n",
    "        # replace with LoRA versions for query and value\n",
    "        block.attention.attention.query = LoRALinear(d, d, bias=False, r=r)\n",
    "        block.attention.attention.value = LoRALinear(d, d, bias=False, r=r)\n",
    "\n",
    "inject_lora_in_vit(model, r=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074e2c2-1af7-4e1a-bfbd-3ef283280fd5",
   "metadata": {},
   "source": [
    "Injecting the LoRA can be seen in the model where the linear layers changed to the `LoRALinear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5e4732ec-e445-4517-97a7-bcf296806b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after LoRA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): LoRALinear()\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): LoRALinear()\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model after LoRA\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a5ef9d59-e14e-46a7-b57e-62c6646a5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") # ViTFeatureExtractor will be depricated\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(feature_extractor.image_mean, feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "train_ds = CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "test_ds  = CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e6102c48-ecb3-43e0-8302-9045b165a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125 x 16 samples.\n",
      "625 x 16 samples.\n"
     ]
    }
   ],
   "source": [
    "print(str(len(train_loader)) + \" x 16 samples.\\n\" + str(len(test_loader))+ \" x 16 samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0cfbc9d8-131a-45db-875d-84b93b01d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    for xb, yb in tqdm(loader):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=xb).logits\n",
    "        loss = F.cross_entropy(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(pixel_values=xb).logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e760e40-08ac-4f65-8b0a-37ee0b92d696",
   "metadata": {},
   "source": [
    "Run both training cells below when large GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "516e7f00-01fd-4741-a53b-e75e923c82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_full = ViTForImageClassification.from_pretrained(\n",
    "#     \"google/vit-base-patch16-224-in21k\", num_labels=10\n",
    "# ).to(device)\n",
    "# opt_full = optim.Adam(model_full.parameters(), lr=3e-5)\n",
    "\n",
    "# train_one_epoch(model_full, train_loader, opt_full, device)\n",
    "# acc_full = evaluate(model_full, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0d344dfd-256e-4f1f-910a-9cff21c8000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lora = ViTForImageClassification.from_pretrained(\n",
    "#     \"google/vit-base-patch16-224-in21k\", num_labels=10\n",
    "# )\n",
    "# inject_lora_in_vit(model_lora, r=4)\n",
    "\n",
    "# # Freeze everything except LoRA\n",
    "# for name, param in model_lora.named_parameters():\n",
    "#     if \"lora_\" not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# model_lora = model_lora.to(device)\n",
    "# opt_lora = optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=3e-5)\n",
    "\n",
    "# train_one_epoch(model_lora, train_loader, opt_lora, device)\n",
    "# acc_lora = evaluate(model_lora, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
