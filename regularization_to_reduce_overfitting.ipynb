{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15960a48-bc8b-4535-8c06-c17925ae88a1",
   "metadata": {},
   "source": [
    "Exploring different regularization techniques for machine learning to overcome overfitting issue using both Scikit-Learn for ML projects and PyTorch for deep learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908fab1-48b4-4861-b8fe-038266eb757f",
   "metadata": {},
   "source": [
    "Some useful terms:\n",
    "* Underfitting - Doesn't fit the training set well (**high bias**).\n",
    "* Generalization - Fits training set pretty well.\n",
    "* Overfit: Fits the training set extremely well (**high variance**) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d75b8-3b12-4097-a08c-f3357588a50b",
   "metadata": {},
   "source": [
    "One simple way to address overfitting in machine learning is:\n",
    "* Through collecting more training data.\n",
    "* `Feature Selection` to include/exclude. Many features with insufficient data will result in overfitting. Feature selection has some disadvantages as the algorithm is excluding some features that could have meaningful information or proportional to the output.\n",
    "* Regularization: gentely reducing some features impact (thus reducing size of parameters) by restricting them without directly eliminating them by setting them equal to zero (as what happens in feature selection), therefor keeping all features with some having smaller effect. Note that it is usually not encouraged to minimize the `bias  (b)` using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d608d020-09f5-443d-8248-08a22a005517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03f9f6-c893-4ead-9c17-8e451537b2f5",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69051a2-2cb9-4e1b-9312-75e355330676",
   "metadata": {},
   "source": [
    "The `sigmoid` function:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "896ea1b6-ea92-4f25-9d75-9a558424ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid in numpy\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A scalar or numpy array of any size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     g : array_like\n",
    "         sigmoid(z)\n",
    "    \"\"\"\n",
    "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "    g = 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba170d-b391-4d87-af75-caf3c62b51c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2caca3a-ccf4-4187-9c37-60b92e90aa6d",
   "metadata": {},
   "source": [
    "##### Cost function for regularized linear regression\n",
    "\n",
    "The equation for the cost function regularized **linear** regression is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ \n",
    "\n",
    "\n",
    "Compare this to the cost function without regularization, which is of the form:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "\n",
    "The difference is the regularization term,  <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "    \n",
    "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.\n",
    "\n",
    "Below is an implementation of equations (1) and (2). Note that this uses a *standard pattern for this course*,   a `for loop` over all `m` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708d5d65-3957-4069-80fa-3d69ff4cf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
    "        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n",
    "    cost = cost / (2 * m)                                              #scalar  \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cc4b0d-e228-491f-9196-a9ed0508c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09431947-f80b-4b38-be6b-225af13fb987",
   "metadata": {},
   "source": [
    "##### Cost function for regularized logistic regression\n",
    "For regularized **logistic** regression, the cost function is of the form\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$ \n",
    "\n",
    "Compare this to the cost function without regularization:\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n",
    "\n",
    "As was the case in linear regression above, the difference is the regularization term, which is    <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "\n",
    "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c616382f-885c-4611-b5a8-0e8204e4f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de9e2a4a-c7f8-4492-835a-da5b50dce296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6117a8-09ea-44d1-b3ec-a9a5b41ae4ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Gradient descent with regularization\n",
    "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
    "\n",
    "What changes with regularization is computing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e612e2-328e-49d7-a0ef-f2671de4878d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Computing the Gradient with regularization (both linear/logistic)\n",
    "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m is the number of training examples in the data set      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
    "\n",
    "      \n",
    "* For a  <span style=\"color:blue\"> **linear** </span> regression model  \n",
    "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "* For a <span style=\"color:blue\"> **logistic** </span> regression model  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    where $g(z)$ is the sigmoid function:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n",
    "The term which adds regularization is  the <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e754b9b-c929-4698-a723-85c86effc443",
   "metadata": {},
   "source": [
    "<b>Gradient function for regularized linear regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10e03c5-0a2f-4af8-8df9-f9252c12c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39439c2f-2d90-4730-aed2-10506379f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425726\n",
      "Regularized dj_dw:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84490180-7d17-4a7f-926f-c39bc8ed6ceb",
   "metadata": {},
   "source": [
    "<b>Gradient function for regularized logistic regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b511ee69-fdf0-4d84-81d4-81f2e3585b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbcb895b-8854-4ae0-bd28-8271f16a8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab486a-1931-4c37-b931-8f040a8a9536",
   "metadata": {},
   "source": [
    "#### Different types of regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b64f01-27c3-4328-b8a4-220fec2e6545",
   "metadata": {},
   "source": [
    "##### ℓ₂ (Ridge) Regularization / Weight Decay\n",
    "Similar to the implemented examples above for linear and logistic regressions, it adds a penalty proportional to the square of each weight → discourages large weights, yielding smoother functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a5ee7-9742-47ec-a5a5-8375dc6b07c9",
   "metadata": {},
   "source": [
    "$Loss = Error(Y - \\widehat{Y}) +  \\lambda \\sum_1^n w_i^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34090b30-5513-43db-868b-caba488f1e93",
   "metadata": {},
   "source": [
    "From `scikit-learn` documentation:\n",
    "\n",
    "```python\n",
    "\n",
    "Parameters: \n",
    "\n",
    "1. penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’\n",
    "    Specify the norm of the penalty:\n",
    "    * None: no penalty is added;\n",
    "    * 'l2': add a L2 penalty term and it is the default choice;\n",
    "    * 'l1': add a L1 penalty term;\n",
    "    * 'elasticnet': both L1 and L2 penalty terms are added.\n",
    "        \n",
    "    Some penalties may not work with some solvers. See the parameter solver in the documentation, to know the compatibility between the penalty and solver.\n",
    "\n",
    "2. Cfloat, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e8cc6ea-fa5a-46b8-b2cf-5ed35d7ff29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n",
      "(750, 20) (750,)\n"
     ]
    }
   ],
   "source": [
    "# In scikit‑learn (e.g. logistic regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a57ad8f-1def-4748-9366-4e234811e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 1/λ; smaller C → stronger ℓ₂ penalty\n",
    "model_no_reg = LogisticRegression(penalty=None, solver='saga', max_iter=10000).fit(X_train, y_train)\n",
    "model_l2    = LogisticRegression(penalty='l2', C=1.0,    solver='saga', max_iter=10000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f07e0a2-56c8-430e-92be-9d8853ac1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reg acc: 0.968\n",
      "L2   reg acc: 0.968\n"
     ]
    }
   ],
   "source": [
    "print(\"No reg acc:\",  accuracy_score(y_val, model_no_reg.predict(X_val)))\n",
    "print(\"L2   reg acc:\", accuracy_score(y_val, model_l2   .predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659031c7-1402-4002-ac03-2304bb5e8234",
   "metadata": {},
   "source": [
    "From Pytorch, some optimizers have built in weight_decay, as `optim.SGD` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e146c2-8a94-401c-97c4-57f39cea7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(20, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 2)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# weight_decay is λ\n",
    "opt_no_decay = optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "opt_decay    = optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7709f-a25c-4be8-8557-9847b17de3ac",
   "metadata": {},
   "source": [
    "##### ℓ₁ (Lasso) Regularization\n",
    "Adds a penalty proportional to the absolute value of weights → yields sparse solutions (many weights zero). L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L). This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a87daa-e6fc-4b0e-8899-8cb60ef31373",
   "metadata": {},
   "source": [
    "$Loss = Error(Y - \\widehat{Y}) + \\lambda \\sum_1^n |w_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d3a41-2dbf-4c35-8dfb-78b7721929d4",
   "metadata": {},
   "source": [
    "Using `scikit-learn`, we can directly use the l1 penalty in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b1a3e2-b5be-40db-8802-e5d3637e84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1   reg acc: 0.968\n",
      "Number of zero weights: 3\n"
     ]
    }
   ],
   "source": [
    "# solver='saga' supports l1 penalty\n",
    "model_l1 = LogisticRegression(penalty='l1', C=0.5, solver='saga', max_iter=10000).fit(X_train, y_train)\n",
    "\n",
    "print(\"L1   reg acc:\", accuracy_score(y_val, model_l1.predict(X_val)))\n",
    "\n",
    "# inspect sparsity\n",
    "print(\"Number of zero weights:\", (model_l1.coef_ == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0c55c-f43e-4fef-a984-d217943146da",
   "metadata": {},
   "source": [
    "PyTorch doesn't support a built-in L1 regularization, so we can implement it in the training loop as followin:\n",
    "\n",
    "```python\n",
    "\n",
    "lambda_l1 = 1e-4\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    logits = model(X_batch)\n",
    "    loss   = criterion(logits, y_batch)\n",
    "    \n",
    "    # ℓ1 penalty\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    loss   += lambda_l1 * l1_norm\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0af4b-e63c-49e1-9067-01990fd16aad",
   "metadata": {},
   "source": [
    "##### Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306728f-e9e7-4348-9591-ff30d9943056",
   "metadata": {},
   "source": [
    "Randomly zeroes a fraction p of activations each forward pass during training; at test time, uses all activations scaled appropriately. This prevents co‑adaptation of neurons. Dropout usually slows convergence slightly but reduces overfitting (training vs. validation gap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8bc7279-218a-4280-b5ed-72ceea06a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),   # 50% dropout\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = NetWithDropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d02a2-f8c9-4168-a2ae-fa156ac745ca",
   "metadata": {},
   "source": [
    "##### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf33486-7c89-460b-88c7-5d9b0a39116e",
   "metadata": {},
   "source": [
    "During training, monitor validation loss (or accuracy) and stop when it hasn’t improved for k epochs, then roll back to the best weights. For PyTorch, use a custom made EarlyStopping class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
